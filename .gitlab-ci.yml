#
# NOTE: Any new jobs not related to the scheduled testing pipeline should have the following
# included as the first rule to prevent them being pulled in to the scheduled pipeline:
#   - if: $CI_PIPELINE_SOURCE == "schedule"
#     when: never

variables:
  NSPECT_ID: NSPECT-K66V-JJ0K
  DOCKER_IMG: nvmetal-carbide

  FORGE_ADMIN_CLI_IMAGE: nvcr.io/nvidian/nvforge-devel/forge-admin-cli
  APPLICATION_DOCKER_IMAGE: nvcr.io/nvidian/nvforge-devel/nvmetal-carbide
  ARTIFACTS_DOCKER_IMAGE_AARCH64: nvcr.io/nvidian/nvforge-devel/boot-artifacts-aarch64
  ARTIFACTS_DOCKER_IMAGE_X86_64: nvcr.io/nvidian/nvforge-devel/boot-artifacts-x86_64
  MACHINE_VALIDATION_ARTIFACTS_DOCKER_IMAGE: nvcr.io/nvidian/nvforge-devel/machine_validation
  MACHINE_VALIDATION_RUNNER_DOCKER_IMAGE: nvcr.io/nvidian/nvforge-devel/machine-validation-runner

  APPLICATION_DOCKER_IMAGE_PRODUCTION: nvcr.io/nvidian/nvforge/nvmetal-carbide
  ARTIFACTS_DOCKER_IMAGE_AARCH64_PRODUCTION: nvcr.io/nvidian/nvforge/boot-artifacts-aarch64
  ARTIFACTS_DOCKER_IMAGE_X86_64_PRODUCTION: nvcr.io/nvidian/nvforge/boot-artifacts-x86_64
  MACHINE_VALIDATION_ARTIFACTS_DOCKER_IMAGE_PRODUCTION: nvcr.io/nvidian/nvforge/machine_validation
  MACHINE_VALIDATION_RUNNER_DOCKER_IMAGE_PRODUCTION: nvcr.io/nvidian/nvforge/machine-validation-runner

  TEST_API_IMAGE: stg.nvcr.io/rkafbrtf9ezb/test-api

  FF_USE_FASTZIP: "true" # enable fastzip - a faster zip implementation that also supports level configuration.
  ARTIFACT_COMPRESSION_LEVEL: default # can also be set to fastest, fast, slow and slowest. If just enabling fastzip is not enough try setting this to fastest or fast.
  CACHE_COMPRESSION_LEVEL: fastest
  TRANSFER_METER_FREQUENCY: 5s
  USE_GIT_HASH_VERSION: "true" # https://gitlab-master.nvidia.com/nvmetal/gitlab-templates/-/merge_requests/88

  KANIKO_CACHE_ARGS: "--cache=false --cache-copy-layers=true --cache-ttl=24h"
  APT_CACHE_DIR: $CI_PROJECT_DIR/apt
  CACHE_DIRECTORY: $CI_PROJECT_DIR/cache # for mkosi
  LOGNAME: root
  KEA_BIN_PATH: /usr/bin
  KEA_INCLUDE_PATH: /usr/include/kea

  CARGO_INCREMENTAL: 0
  CARGO_HOME: $CI_PROJECT_DIR/cargo
  NEED_HELM: "yes"

  # FF_TIMESTAMPS: "true" # timestamp log output. Currently broken on certain runner versions

  #
  # Container Regsitries for intermediate containers (build & runtime), not for publishing.
  #
  CONTAINER_REGISTRY_AARCH64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8"
  CONTAINER_REGISTRY_X86_64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86-64"


  # Canonical versions - will be overridden if you touch one of the dockerfiles that builds them
  #
  # How to update a build or runtime container.
  #
  # Make one commit where you update the Dockerfile in dev/docker.  Make an MR for it, it will build the new container and push it to urm in the MR.
  #
  # Next, *add* a commit to your MR where you update the relevant docker locations here.
  #
  # If you don't do the second step, merging to production will use the prior container.  I don't know how to solve this, but it's the way it was before
  #
  CONTAINER_BUILD_AARCH64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8/build-container:latest"
  CONTAINER_BUILD_X86_64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86-64/build-container:latest"
  CONTAINER_FORGE_ADMIN_CLI_URM: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/tools/forge-admin-cli:latest"
  CONTAINER_RUNTIME_AARCH64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8/runtime-container:latest"
  CONTAINER_RUNTIME_X86_64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86-64/runtime-container:latest"
  CONTAINER_BUILD_ARTIFACTS_AARCH64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8/build-artifacts-container:latest"
  CONTAINER_BUILD_ARTIFACTS_X86_64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86-64/build-artifacts-container:latest"
  CONTAINER_ARTIFACTS_X86_64: "alpine:latest"
  CONTAINER_ARTIFACTS_AARCH64: "alpine:latest"
  CONTAINER_SITE_AUTO_DEPLOY: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/tools/site-auto-deploy-container:latest"
  CONTAINER_MACHINE_LIFECYCLE_TEST: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/tools/machine-lifecycle-test-container:latest"

include:
  - project: "nvmetal/gitlab-templates"
    ref: main
    file:
      - defaults.gitlab-ci.yml
  - project: pstooling/gitlab-templates
    ref: main
    file: templates/pulse-in-pipeline/ContainerScan.gitlab-ci.yml
  - project: 'pstooling/gitlab-templates'
    ref: main
    file:
      - 'templates/pulse-in-pipeline/Scan.gitlab-ci.yml'

prepare-ci:
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH || $CI_COMMIT_BRANCH =~ /^release\//
    - if: $CI_PIPELINE_SOURCE

build:
  rules:
    - when: never

build-push-helm:
  rules:
    - when: never

build-push-kustomize:
  rules:
    - when: never

notify-security-report:
  rules:
    - when: never

prepare-kustomize-for-scan:
  rules:
    - when: never

build-container:
  rules:
    - when: never

checkmarx-scan-csv:
  rules:
    - when: never

checkov-scan:
  rules:
    - when: never

container-scan:
  rules:
    - when: never

pulse-open-source:
  rules:
    - when: never

export-docker-json-config:
  when: always

#
# Build the container that builds our software
#
build-build-container-x86_64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  extends: build-container
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.build-container-x86_64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/x86-64/build-container
  after_script:
    - echo "CONTAINER_BUILD_X86_64=$CONTAINER_REGISTRY_X86_64/build-container:$VERSION" > build-container-x86_64.env
    - echo "CONTAINER_BUILD_X86_64_WAS_BUILT=true" >> build-container-x86_64.env
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.build-container-x86_64
  artifacts:
    reports:
      dotenv: build-container-x86_64.env

#
# ARM version uses kaniko instead of extending build-container because devops-tools doesn't have an ARM version
#
build-build-container-aarch64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.build-container-aarch64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/arm64v8/build-container
  before_script: |
    mkdir -p /kaniko/.docker
    echo "$DOCKER_AUTH_CONFIG" > /kaniko/.docker/config.json
  script: |
    set -xeuo pipefail
    /kaniko/executor --context $CI_PROJECT_DIR/dev/docker --dockerfile $DOCKER_FILE --destination $BUILD_DOCKER_IMAGE:$VERSION
  tags:
    - aarch64
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.build-container-aarch64
  after_script:
    - echo "CONTAINER_BUILD_AARCH64=$CONTAINER_REGISTRY_AARCH64/build-container:$VERSION" > build-container-aarch64.env
    - echo "CONTAINER_BUILD_AARCH64_WAS_BUILT=true" >> build-container-aarch64.env
  artifacts:
    reports:
      dotenv: build-container-aarch64.env

#
# Build the container that builds our software
#
build-artifacts-build-container-x86_64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  extends: build-container
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.build-artifacts-container-x86_64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/x86-64/build-artifacts-container
  after_script:
    - echo "CONTAINER_BUILD_ARTIFACTS_X86_64=$CONTAINER_REGISTRY_X86_64/build-artifacts-container:$VERSION" > build-artifacts-container-x86_64.env
    - echo "CONTAINER_BUILD_ARTIFACTS_X86_64_WAS_BUILT=true" >> build-artifacts-container-x86_64.env
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.build-artifacts-container-x86_64
  artifacts:
    reports:
      dotenv: build-artifacts-container-x86_64.env

#
# ARM version uses kaniko instead of extending build-container because devops-tools doesn't have an ARM version
#
build-artifacts-build-container-aarch64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.build-artifacts-container-aarch64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/arm64v8/build-artifacts-container
  before_script: |
    mkdir -p /kaniko/.docker
    echo "$DOCKER_AUTH_CONFIG" > /kaniko/.docker/config.json
  script: |
    set -xeuo pipefail
    /kaniko/executor --context $CI_PROJECT_DIR/dev/docker --dockerfile $DOCKER_FILE --destination $BUILD_DOCKER_IMAGE:$VERSION
  tags:
    - aarch64
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.build-artifacts-container-aarch64
  after_script:
    - echo "CONTAINER_BUILD_ARTIFACTS_AARCH64=$CONTAINER_REGISTRY_AARCH64/build-artifacts-container:$VERSION" > build-artifacts-container-aarch64.env
    - echo "CONTAINER_BUILD_ARTIFACTS_AARCH64_WAS_BUILT=true" >> build-artifacts-container-aarch64.env
  artifacts:
    reports:
      dotenv: build-artifacts-container-aarch64.env

#
# Build the container that will host the finalized carbide code.
#
build-runtime-container-x86_64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  extends: build-container
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.runtime-container-x86_64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/x86-64/runtime-container
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.runtime-container-x86_64
  after_script:
    - echo "CONTAINER_RUNTIME_X86_64=$CONTAINER_REGISTRY_X86_64/runtime-container:$VERSION" > runtime-container-x86_64.env
    - echo "CONTAINER_RUNTIME_X86_64_WAS_BUILT==true" >> runtime-container-x86_64.env
  artifacts:
    reports:
      dotenv: runtime-container-x86_64.env

#
# ARM version uses kaniko instead of extending build-container because devops-tools doesn't have an ARM version
#
build-runtime-container-aarch64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.runtime-container-aarch64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/arm64v8/runtime-container
  tags:
    - aarch64
  script: /bin/true
  rules:
    - when: never # TODO - we don't run carbide on ARM yet
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
#    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
#    - changes:
#        - dev/docker/Dockerfile.runtime-container-aarch64
  after_script:
    - echo "CONTAINER_RUNTIME_AARCH64=$CONTAINER_REGISTRY_AARCH64/runtime-container:$VERSION" > runtime-container-aarch64.env
    - echo "CONTAINER_RUNTIME_AARCH64_WAS_BUILT=true" >> runtime-container-aarch64.env
  artifacts:
    reports:
      dotenv: runtime-container-aarch64.env

#
# When a new build/runtime/artifacts container is built, and we're on the
# trunk/main branch, then make sure we tag it with 'latest' so that subsequent
# jobs will use it.
#
tag-build-and-runtime-containers:
  stage: publish
  image:
    name: gcr.io/go-containerregistry/crane:debug
    entrypoint: [""]
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-build-container-x86_64
      optional: true
    - job: build-build-container-aarch64
      optional: true
    - job: build-runtime-container-x86_64
      optional: true
    - job: build-runtime-container-aarch64
      optional: true
    - job: build-artifacts-build-container-x86_64
      optional: true
    - job: build-artifacts-build-container-aarch64
      optional: true
  allow_failure: true
  before_script: |
    mkdir -p /root/.docker
    cp .docker/config.json /root/.docker/config.json
  script: |
    set -xeuo pipefail

    if [ ! -z "${CONTAINER_BUILD_X86_64_WAS_BUILT:-}" ]; then
      crane copy "${CONTAINER_BUILD_X86_64}" "${CONTAINER_REGISTRY_X86_64}/build-container:latest"
    fi

    if [ ! -z "${CONTAINER_BUILD_AARCH64_WAS_BUILT:-}" ]; then
      crane copy "${CONTAINER_BUILD_AARCH64}" "${CONTAINER_REGISTRY_AARCH64}/build-container:latest"
    fi

    if [ ! -z "${CONTAINER_BUILD_ARTIFACTS_X86_64_WAS_BUILT:-}" ]; then
      crane copy "${CONTAINER_BUILD_ARTIFACTS_X86_64}" "${CONTAINER_REGISTRY_X86_64}/build-artifacts-container:latest"
    fi

    if [ ! -z "${CONTAINER_BUILD_ARTIFACTS_AARCH64_WAS_BUILT:-}" ]; then
      crane copy "${CONTAINER_BUILD_ARTIFACTS_AARCH64}" "${CONTAINER_REGISTRY_AARCH64}/build-artifacts-container:latest"
    fi

    if [ ! -z "${CONTAINER_RUNTIME_X86_64_WAS_BUILT:-}" ]; then
      crane copy "${CONTAINER_RUNTIME_X86_64}" "${CONTAINER_REGISTRY_X86_64}/runtime-container:latest"
    fi

    if [ ! -z "${CONTAINER_RUNTIME_AARCH64_WAS_BUILT:-}" ]; then
      crane copy  "${CONTAINER_RUNTIME_AARCH64}" "${CONTAINER_REGISTRY_AARCH64}/runtime-container:latest"
    fi

  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/ || $CI_COMMIT_BRANCH =~ /^release\//

#
# Build the final container that releases our code
#
build-release-container-x86_64:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
    - job: build-build-container-x86_64
      optional: true
    - job: build-runtime-container-x86_64
      optional: true
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-container-x86_64
    FF_TIMESTAMPS: true
  script:
    - docker build -t intermediate-container-$CI_COMMIT_SHORT_SHA -f $DOCKER_FILE --build-arg VERSION=$VERSION --build-arg CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA --build-arg CONTAINER_RUNTIME_X86_64=$CONTAINER_RUNTIME_X86_64 --build-arg CONTAINER_BUILD_X86_64=$CONTAINER_BUILD_X86_64 .
    - |
        if test "$DEPLOY_BUILT_CONTAINER" = "true"; then
          echo "Deploying build container: intermediate-container-$CI_COMMIT_SHORT_SHA to $APPLICATION_DOCKER_IMAGE:$VERSION"
          docker tag "intermediate-container-$CI_COMMIT_SHORT_SHA" "$APPLICATION_DOCKER_IMAGE:$VERSION"
          docker push "$APPLICATION_DOCKER_IMAGE:$VERSION"
          docker rmi "$APPLICATION_DOCKER_IMAGE:$VERSION"
        else
          echo "Skipping deployment of intermediate-container-$CI_COMMIT_SHORT_SHA"
        fi
    - docker rmi intermediate-container-$CI_COMMIT_SHORT_SHA
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH || $CI_COMMIT_BRANCH =~ /^release\//  # Run for default and those starting with "release/"
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"
  after_script:
    - echo "CONTAINER_RELEASE_X86_64=$APPLICATION_DOCKER_IMAGE:$VERSION" > release-container-x86_64.env
  artifacts:
    reports:
      dotenv: release-container-x86_64.env


build-container-forge-cli-x86_64:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-build-artifacts-container-x86_64
      optional: true
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-forge-cli
  dependencies:
    - prepare-ci
    - export-docker-json-config
  tags:
    - x86_64
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  before_script: |
    set -euo pipefail
    mkdir -p /kaniko/.docker
    cp .docker/config.json /kaniko/.docker/config.json
    export VAULT_JWT_ROLE=$VAULT_JWT_ROLE_FORGE
    export VAULT_JWT_PATH=$VAULT_JWT_PATH_FORGE
    export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$VAULT_JWT_TOKEN)"
  script:
    - >-
      /kaniko/executor --context "${CI_PROJECT_DIR}" --dockerfile "${DOCKER_FILE}" --build-arg "CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA" --build-arg "CONTAINER_BUILD=$CONTAINER_BUILD_ARTIFACTS_X86_64" --destination "${FORGE_ADMIN_CLI_IMAGE}-${VERSION}:amd64"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      variables:
        KANIKO_NO_PUSH: "false"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        KANIKO_NO_PUSH: "true"

build-container-forge-cli-aarch64:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-build-artifacts-container-aarch64
      optional: true
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-forge-cli
  dependencies:
    - prepare-ci
    - export-docker-json-config
  tags:
    - aarch64
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  before_script: |
    set -euo pipefail
    mkdir -p /kaniko/.docker
    cp .docker/config.json /kaniko/.docker/config.json
    export VAULT_JWT_ROLE=$VAULT_JWT_ROLE_FORGE
    export VAULT_JWT_PATH=$VAULT_JWT_PATH_FORGE
    export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$VAULT_JWT_TOKEN)"
  script:
    - >-
      /kaniko/executor --context "${CI_PROJECT_DIR}" --dockerfile "${DOCKER_FILE}" --build-arg "CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA" --build-arg "CONTAINER_BUILD=$CONTAINER_BUILD_ARTIFACTS_AARCH64" --destination "${FORGE_ADMIN_CLI_IMAGE}-${VERSION}:arm64"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      variables:
        KANIKO_NO_PUSH: "false"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        KANIKO_NO_PUSH: "true"

merge-manifests-forge-cli:
  stage: build
  # all containers must be build before merging them
  # alternatively the job may be configured to run in a later stage
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-container-forge-cli-aarch64
      artifacts: false
    - job: build-container-forge-cli-x86_64
      artifacts: false
  before_script: |
    mkdir -p /kaniko/.docker
    cp .docker/config.json /kaniko/.docker/config.json
  tags:
    # may run on any architecture supported by manifest-tool image
    - x86_64
  image:
    name: mplatform/manifest-tool:alpine
    entrypoint: [""]
  script:
    - >-
      manifest-tool --docker-cfg .docker/config.json push from-args --platforms linux/amd64,linux/arm64 --template "${FORGE_ADMIN_CLI_IMAGE}-${VERSION}:ARCH" --tags latest --target "${FORGE_ADMIN_CLI_IMAGE}:${VERSION}"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      when: never

# Copy the final multi-arch container to urm for use in QA automation
copy-forge-admin-cli-to-urm:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: merge-manifests-forge-cli
      optional: false
    - job: get-crane
      optional: false
  image:
    name: gcr.io/go-containerregistry/crane:debug
    entrypoint: [""]
  tags:
    - x86_64
  before_script:
    - mkdir -p /root/.docker
    - cp .docker/config.json /root/.docker/config.json
  script:
    - echo "Copying ${FORGE_ADMIN_CLI_IMAGE}:latest to ${CONTAINER_FORGE_ADMIN_CLI_URM}"
    - ./crane copy "${FORGE_ADMIN_CLI_IMAGE}:latest" "${CONTAINER_FORGE_ADMIN_CLI_URM}"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      when: never


#
# Build the final container that releases our code
#
.build-boot-artifacts:
  stage: build
  variables:
    FORGE_CA: "prod"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - when: always

build-boot-artifacts-x86_64:
  extends: .build-boot-artifacts
  needs:
    - job: prepare-ci
      optional: false
    - job: build-artifacts-build-container-x86_64
      optional: true
  image:
    name: $CONTAINER_BUILD_ARTIFACTS_X86_64
  script:
    - cargo make --cwd pxe build-boot-artifacts-x86_64-ci
  artifacts:
    paths:
      - pxe/static/blobs/internal/x86_64/ipxe.efi
      - target/debug/forge-admin-cli
      - target/debug/forge-scout
    when: always
    expire_in: 1 day

build-boot-artifacts-aarch64:
  extends: .build-boot-artifacts
  needs:
    - job: prepare-ci
      optional: false
    - job: build-artifacts-build-container-aarch64
      optional: true
  tags:
    - aarch64 # not shell because no mkosi aarch64 image
  image:
    name: $CONTAINER_BUILD_ARTIFACTS_AARCH64
  script:
    - cargo make --cwd pxe build-boot-artifacts-aarch64-ci
  artifacts:
    paths:
      - pxe/static/blobs/internal/aarch64/ipxe.efi
      - pxe/static/blobs/internal/aarch64/carbide.efi
      - pxe/static/blobs/internal/aarch64/carbide.root
      - pxe/static/blobs/internal/apt
      - pxe/static/blobs/internal/firmware
      - target/debug/forge-admin-cli
      - target/debug/forge-scout
    when: always
    expire_in: 1 day

# This just builds the mkosi container and uses artifacts from a previous build
build-boot-artifacts-ephemeral-image-x86-64:
  extends: .build-boot-artifacts
  needs:
    - job: prepare-ci
      optional: false
    - job: build-boot-artifacts-x86_64
      optional: false
  dependencies:
    - build-boot-artifacts-x86_64
  tags:
    - shell # mkosi cannot run in docker :(
    - ukify
    - mkosi
  image:
    name: $CONTAINER_BUILD_ARTIFACTS_AARCH64
  script:
    - cargo make --cwd pxe create-ephemeral-image-x86_64-ci
  after_script:
    - "sudo chown -R gitlab-runner: /home/gitlab-runner/builds/"
  artifacts:
    paths:
      - pxe/static/blobs/internal/x86_64/scout.efi
      - pxe/static/blobs/internal/x86_64/qcow-imager.efi
    when: always
    expire_in: 1 day

build-release-artifacts-x86_64:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
    - job: build-boot-artifacts-x86_64
      optional: false
    - job: build-boot-artifacts-ephemeral-image-x86-64
      optional: false
  dependencies:
    - build-boot-artifacts-ephemeral-image-x86-64
    - build-boot-artifacts-x86_64
    - prepare-ci
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-artifacts-x86_64
  script:
    - docker build -t intermediate-artifacts-$CI_COMMIT_SHORT_SHA -f $DOCKER_FILE --build-arg VERSION=$VERSION --build-arg CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA --build-arg CONTAINER_RUNTIME_X86_64=$CONTAINER_ARTIFACTS_X86_64 .
    - |
        if test "$DEPLOY_BUILT_CONTAINER" = "true"; then
          echo "Deploying build container: intermediate-artifacts-$CI_COMMIT_SHORT_SHA to $ARTIFACTS_DOCKER_IMAGE_X86_64:$VERSION"
          docker tag "intermediate-artifacts-$CI_COMMIT_SHORT_SHA" "$ARTIFACTS_DOCKER_IMAGE_X86_64:$VERSION"
          docker push "$ARTIFACTS_DOCKER_IMAGE_X86_64:$VERSION"
        else
          echo "Skipping deployment of intermediate-artifacts-$CI_COMMIT_SHORT_SHA"
        fi
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/ || $CI_COMMIT_BRANCH =~ /^release\//
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"

build-release-machine-validation-artifacts-x86_64:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
    - job: build-release-machine-validation-runner
      optional: false
  dependencies:
    - prepare-ci
    - build-release-machine-validation-runner
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.machine-validation-config
  script:
    - docker build -t intermediate-artifacts-$CI_COMMIT_SHORT_SHA -f $DOCKER_FILE --build-arg VERSION=$VERSION --build-arg CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA --build-arg CONTAINER_RUNTIME_X86_64=$CONTAINER_ARTIFACTS_X86_64 .
    - |
        if test "$DEPLOY_BUILT_CONTAINER" = "true"; then
          echo "Deploying build container: intermediate-artifacts-$CI_COMMIT_SHORT_SHA to $MACHINE_VALIDATION_ARTIFACTS_DOCKER_IMAGE:$VERSION"
          docker tag "intermediate-artifacts-$CI_COMMIT_SHORT_SHA" "$MACHINE_VALIDATION_ARTIFACTS_DOCKER_IMAGE:$VERSION"
          docker push "$MACHINE_VALIDATION_ARTIFACTS_DOCKER_IMAGE:$VERSION"
        else
          echo "Skipping deployment of intermediate-artifacts-$CI_COMMIT_SHORT_SHA"
        fi
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/ || $CI_COMMIT_BRANCH =~ /^release\//
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"

build-release-machine-validation-runner:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
  dependencies:
    - prepare-ci
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.machine-validation-runner
  script:
    - docker build -t intermediate-artifacts-$CI_COMMIT_SHORT_SHA -f $DOCKER_FILE --build-arg VERSION=$VERSION --build-arg CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA --build-arg CONTAINER_RUNTIME_X86_64=$CONTAINER_ARTIFACTS_X86_64 .
    - docker tag "intermediate-artifacts-$CI_COMMIT_SHORT_SHA" $MACHINE_VALIDATION_RUNNER_DOCKER_IMAGE_PRODUCTION:latest
    - docker save --output common/machine-validation/images/machine-validation-runner.tar $MACHINE_VALIDATION_RUNNER_DOCKER_IMAGE_PRODUCTION:latest
    - |
        echo "In case if there is registry access. May not be required Check later"
        if test "$DEPLOY_BUILT_CONTAINER" = "true"; then
          echo "Deploying build container: intermediate-artifacts-$CI_COMMIT_SHORT_SHA to $MACHINE_VALIDATION_RUNNER_DOCKER_IMAGE:$VERSION"
          docker tag "intermediate-artifacts-$CI_COMMIT_SHORT_SHA" "$MACHINE_VALIDATION_RUNNER_DOCKER_IMAGE:$VERSION"
          docker push "$MACHINE_VALIDATION_RUNNER_DOCKER_IMAGE:$VERSION"
        else
          echo "Skipping deployment of intermediate-artifacts-$CI_COMMIT_SHORT_SHA"
        fi
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/ || $CI_COMMIT_BRANCH =~ /^release\//
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"
  artifacts:
    paths:
      - common/machine-validation/images
    when: always
    expire_in: 1 day

# This just builds the mkosi container and uses artifacts from a previous build
build-boot-artifacts-ephemeral-image-aarch64:
  extends: .build-boot-artifacts
  allow_failure: true # TEMP
  needs:
    - job: prepare-ci
      optional: false
    - job: build-boot-artifacts-aarch64
      optional: false
  dependencies:
    - build-boot-artifacts-aarch64
  tags:
    - aarch64-shell
  script:
    - cargo make --cwd pxe create-ephemeral-image-aarch64-ci
  after_script:
    - "sudo chown -R gitlab-runner: /home/gitlab-runner/builds/"
  artifacts:
    paths:
      - pxe/static/blobs/internal/aarch64/scout.efi
      # - pxe/static/blobs/internal/aarch64/qcow-imager.efi
    when: always
    expire_in: 1 day

build-release-artifacts-aarch64:
  stage: build
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  tags:
    - x86_64 # The AARCH64 artifacts are still packaged into the X86_64 version of the image because we execute commands in it on an X86_64 host
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-runtime-container-x86_64
      optional: true
    - job: build-boot-artifacts-aarch64
      optional: false
    - job: build-boot-artifacts-ephemeral-image-aarch64
      optional: false
  dependencies:
    - build-boot-artifacts-aarch64
    - prepare-ci
    - export-docker-json-config
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-artifacts-aarch64
  before_script: |
    mkdir -p /kaniko/.docker
    cp .docker/config.json /kaniko/.docker/config.json
  script: |
    set -euo pipefail
    if test "$DEPLOY_BUILT_CONTAINER" = "true"; then
      echo "Deploying container: $ARTIFACTS_DOCKER_IMAGE_AARCH64:$VERSION"

      # The AARCH64 artifacts are still packaged into the X86_64 version of the image because we execute commands in it on an X86_64 host
      /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $DOCKER_FILE --build-arg "CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA" --build-arg "CONTAINER_RUNTIME_AARCH64=$CONTAINER_ARTIFACTS_X86_64" --destination "$ARTIFACTS_DOCKER_IMAGE_AARCH64:$VERSION"
    else
      echo "Skipping deployment of $ARTIFACTS_DOCKER_IMAGE_AARCH64:$VERSION"

      # The AARCH64 artifacts are still packaged into the X86_64 version of the image because we execute commands in it on an X86_64 host
      /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $DOCKER_FILE --build-arg "CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA" --build-arg "CONTAINER_RUNTIME_AARCH64=$CONTAINER_ARTIFACTS_X86_64" --no-push
    fi
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/ || $CI_COMMIT_BRANCH =~ /^release\//
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"

get-crane:
  stage: .pre
  image:
    name: gcr.io/go-containerregistry/crane:debug
    entrypoint: [""]
  variables:
    PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/busybox:/ko-app
  script:
    - cp $(which crane) .
  artifacts:
    paths:
      - crane
    when: always
    expire_in: 1 day
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - when: always

nspect-source-scan-x86_64:
  stage: .pre
  extends: .scan-with-report-no-fail
  allow_failure: true
  variables:
    PULSE_NSPECT_ID: $NSPECT_ID
    PULSE_REPO_URL: "${CI_PROJECT_URL}.git"
    PULSE_SCAN_PROJECT: "carbide"
    PULSE_SCAN_PROJECT_VERSION: "${CI_COMMIT_BRANCH}"
    PULSE_SCAN_VULNERABILITY_REPORT: "nspect_${CI_COMMIT_BRANCH}_scan_report.json"
    PULSE_SCAN_EXCLUDE_DETECTORS: "Setuptools"
  rules:
    - when: never
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
      variables:
        PULSE_SCAN_OVERRIDE: "true"
        PULSE_REPO_BRANCH: trunk
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event' && $CI_COMMIT_MESSAGE =~ /ci-run-security-scan/

nspect-container-scan-x86_64:
  stage: security
  extends: .scan-dind-override-report
  services: []
  dependencies:
    - get-crane
    - export-docker-json-config
    - build-release-container-x86_64
  needs:
    - job: build-release-container-x86_64
      optional: false
    - job: get-crane
      optional: false
    - job: export-docker-json-config
      optional: false
  variables:
    CONTAINER_IMAGE: $CONTAINER_RELEASE_X86_64
  before_script:
    - mkdir -p /root/.docker
    - cp .docker/config.json /root/.docker/config.json
  script:
    - !reference [.scan_script, validate_requirements]
    - !reference [.scan_script, get_ssa_token]
    - echo "Using $CONTAINER_SCAN_HELPER_VERSION of the container scan helper."
    - ./crane version
    - ./crane pull ${CONTAINER_IMAGE} /tmp/archive.tar
    - pulse-cli -n $NSPECT_ID --ssa $SSA_TOKEN -p .gitlab/runtime-container-policy.json scan --override=true --program-version="Forge-LPR" --is-release=true -i /tmp/archive.tar --sbom json -o
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH|| $CI_COMMIT_BRANCH =~ /^release\//

promote-images-to-production:
  stage: promote
  image:
    name: gcr.io/go-containerregistry/crane:debug
    entrypoint: [""]
  needs:
    - job: build-release-artifacts-aarch64
      optional: false
    - job: build-release-artifacts-x86_64
      optional: false
    - job: build-release-container-x86_64
      optional: false
    - job: nspect-container-scan-x86_64
      optional: false
  when: manual
  before_script: |
    mkdir -p /root/.docker
    cp .docker/config.json /root/.docker/config.json
  script: |
    set -euo pipefail
    crane copy $APPLICATION_DOCKER_IMAGE:$VERSION $APPLICATION_DOCKER_IMAGE_PRODUCTION:$VERSION
    crane copy $ARTIFACTS_DOCKER_IMAGE_AARCH64:$VERSION $ARTIFACTS_DOCKER_IMAGE_AARCH64_PRODUCTION:$VERSION
    crane copy $ARTIFACTS_DOCKER_IMAGE_X86_64:$VERSION $ARTIFACTS_DOCKER_IMAGE_X86_64_PRODUCTION:$VERSION
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_BRANCH =~ /^release\//

license-checks:
  stage: security
  image:
    name: $CONTAINER_BUILD_X86_64
  rules:
    - when: never
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - changes:
        - Cargo.lock
        - .license-config.yml
    - if: $CI_COMMIT_TAG
    - if: $CI_PIPELINE_SOURCE != 'merge_request_event'

pages:
  stage: doc
  needs:
    - job: export-docker-json-config
      optional: false
    - job: build-runtime-container-x86_64
      optional: true
    - job: build-boot-artifacts-aarch64
      optional: false
  image:
    name: $CONTAINER_BUILD_X86_64
  script:
    - cargo make book
  artifacts:
    paths:
      - public
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - changes:
        - book/
    - if: $CI_COMMIT_TAG
    - if: $CI_PIPELINE_SOURCE != 'merge_request_event'

#
# Build and push the generic CI tools image
#
build-ci-tools-container:
  extends: build-container
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      changes:
        - dev/docker/Dockerfile.ci-tools-container-lite
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.ci-tools-container-lite
    BUILD_DOCKER_IMAGE: $CI_TOOLS_IMAGE
  script:
    - docker build --pull --add-host=gitlab-master.nvidia.com:10.120.180.120 --file $DOCKER_FILE --tag $BUILD_DOCKER_IMAGE .
    - docker push $BUILD_DOCKER_IMAGE

#
# Build and push the image that contains the dependencies for job 'scheduled-test:auto-deploy-site-controller'
#
build-site-auto-deploy-container:
  extends: build-container
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      changes:
        - dev/docker/Dockerfile.site-auto-deploy-container
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.site-auto-deploy-container
    BUILD_DOCKER_IMAGE: $CONTAINER_SITE_AUTO_DEPLOY
  script:
    - docker build --pull --add-host=gitlab-master.nvidia.com:10.120.180.120 --file $DOCKER_FILE --tag $BUILD_DOCKER_IMAGE .
    - docker push $BUILD_DOCKER_IMAGE

#
# Build and push the image that contains the dependencies for job 'scheduled-test:machine-lifecycle-test'
#
build-machine-lifecycle-test-container:
  extends: build-container
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      changes:
        - dev/docker/Dockerfile.machine-lifecycle-test-container
        - dev/docker/Dockerfile.release-forge-cli
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.machine-lifecycle-test-container
    BUILD_DOCKER_IMAGE: $CONTAINER_MACHINE_LIFECYCLE_TEST
  script:
    - docker build --pull --add-host=gitlab-master.nvidia.com:10.120.180.120 --file $DOCKER_FILE --tag $BUILD_DOCKER_IMAGE .
    - docker push $BUILD_DOCKER_IMAGE

#
# Generate variables and certs needed by the scheduled test pipeline
#
scheduled-test:prepare:
  stage: .pre
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && ($AUTO_DEPLOY == "true" || $RUN_TESTS == "true")
  retry:
    when:
      - runner_system_failure
    max: 1
  image:
    name: $CI_TOOLS_IMAGE
    entrypoint: [ "" ]
  variables:
    # This variable is specified in GitLab UI > Scheduled Pipelines
    SITE_UNDER_TEST: $SITE_UNDER_TEST
    VAULT_NAMESPACE: ngc-forge
  id_tokens:
    VAULT_JWT_TOKEN:
      aud: $VAULT_ADDR
  before_script:
    - set -euo pipefail
    - export VAULT_JWT_ROLE=$VAULT_JWT_ROLE_FORGE
    - export VAULT_JWT_PATH=$VAULT_JWT_PATH_FORGE
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$VAULT_JWT_TOKEN)"
  script:
    # Parse short site name from full site name
    - export SHORT_SITE_NAME=$(echo "${SITE_UNDER_TEST}" | cut -d '-' -f 2)
    # Generate carbide API URL
    - export CARBIDE_API_URL="api-${SHORT_SITE_NAME}.frg.nvidia.com"
    - echo "SHORT_SITE_NAME=${SHORT_SITE_NAME}" > variables.env
    - echo "CARBIDE_API_URL=${CARBIDE_API_URL}" >> variables.env
    # Generate short-lived carbide API certificate
    - mkdir certs && pushd certs
    - vault write --format json forgeca-production/issue/ci-api-access common_name="${CARBIDE_API_URL}" ttl="6h" > vault_cert_output.json
    - cat vault_cert_output.json | jq -r .data.certificate > ci-api-user.crt
    - cat vault_cert_output.json | jq -r .data.private_key > ci-api-user
    - popd
  artifacts:
    reports:
      dotenv: variables.env
    paths:
      - certs/

#
# Auto-deploy the latest versions of carbide and ssh-console to the site under test by updating the `forged` repo
#
scheduled-test:auto-deploy-site-controller:
  stage: dev-deploy
  needs:
    - job: export-docker-json-config
    - job: scheduled-test:prepare
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && $AUTO_DEPLOY == "true"
  image:
    name: $CONTAINER_SITE_AUTO_DEPLOY
    entrypoint: [ "" ]
  variables:
    VAULT_NAMESPACE: ngc-forge
  id_tokens:
    VAULT_JWT_TOKEN:
      aud: $VAULT_ADDR
  before_script:
    - set -euo pipefail
    - mkdir -p /root/.docker
    - cp .docker/config.json /root/.docker/config.json
    - export VAULT_JWT_ROLE=$VAULT_JWT_ROLE_FORGE
    - export VAULT_JWT_PATH=$VAULT_JWT_PATH_FORGE
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$VAULT_JWT_TOKEN)"
  script:
    - bash .gitlab/update-site-controller.sh
  artifacts:
    reports:
      dotenv: versions.env

#
# Verify the running version of carbide-api on the site under test is the version that was deployed
#
scheduled-test:verify-site-controller-version:
  stage: dev-deploy
  needs:
    - job: scheduled-test:prepare
    - job: scheduled-test:auto-deploy-site-controller
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && $AUTO_DEPLOY == "true"
  retry:
    when:
      - runner_system_failure
    max: 1
  image:
    name: $CONTAINER_FORGE_ADMIN_CLI_URM
    entrypoint: [ "" ]
  variables:
    CLIENT_CERT_PATH: certs/ci-api-user.crt
    CLIENT_KEY_PATH: certs/ci-api-user
  script:
    - set -euo pipefail
    - echo "Checking carbide-api is version ${LATEST_COMMON_VERSION}"
    - DEPLOYED_VERSION=$(forge-admin-cli --format json -c https://${CARBIDE_API_URL} version | jq -r '.build_version')
    - |
      if [ ${DEPLOYED_VERSION} != ${LATEST_COMMON_VERSION} ]; then
        echo "Expected version ${LATEST_COMMON_VERSION} on site ${SITE_UNDER_TEST}, actual version is ${DEPLOYED_VERSION}";
        exit 1
      fi

#
# Run cloud-api sanity tests against the site under test.
# The image is built in repo: https://gitlab-master.nvidia.com/cloud-service-qa/test-image
# The test code is in repo: https://gitlab-master.nvidia.com/compute-swqa/cuda-automation-infra/tesla_automation_test/-/tree/main/Forge/tests/Forge_Testing/CommonTestSteps/API
# NOTE: Currently supports pdx-demo1 and reno-dev4
#
scheduled-test:cloud-api-tests:
  stage: test
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && $AUTO_DEPLOY == "false" && $RUN_TESTS =="true"
      when: never  # Remove when tests are fixed
    - if: $CI_PIPELINE_SOURCE == "schedule" && $AUTO_DEPLOY == "true" && $RUN_TESTS =="false"
      when: never
    - if: $CI_PIPELINE_SOURCE == "schedule" && $AUTO_DEPLOY == "true" && $RUN_TESTS == "true"
      needs:
        - job: scheduled-test:verify-site-controller-version
      when: never  # Remove when tests are fixed
  timeout: 2h
  retry:
    when:
      - runner_system_failure
    max: 1
  variables:
    PROJECT: forge
    COMPONENT: api
    SCOPE: site-sanity
    VAULT_NAMESPACE: ngc-forge
  id_tokens:
    VAULT_JWT_TOKEN:
      aud: $VAULT_ADDR
  image:
    name: $TEST_API_IMAGE
    entrypoint: [ "" ]
  before_script:
    - set -euo pipefail
    - export VAULT_JWT_ROLE=$VAULT_JWT_ROLE_FORGE
    - export VAULT_JWT_PATH=$VAULT_JWT_PATH_FORGE
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$VAULT_JWT_TOKEN)"
    - |
      export API_TOKEN
      if [[ $SITE_UNDER_TEST == "pdx-demo1" ]]; then
        API_TOKEN=$(vault kv get -field nvcr secrets/forge/tokens)
        export API_ENDPOINT="https://api.ngc.nvidia.com"
        export FORGE_URL="https://forge.ngc.nvidia.com/"
        export NGC_AUTH="ttps://ngc.authn.nvidia.com/token?service=ngc&scope=group/ngc"
        export PROVIDER_ORG="forge-prime-provider"
        export PROVIDER_ORG_ID="qpbftykv9atm"
        export TENANT_ORG="forge-tenant-dev"
        export TENANT_ORG_ID="i1k1exmxlqr1"
      elif [[ $SITE_UNDER_TEST == "reno-dev4" ]]; then
        API_TOKEN=$(vault kv get -field stg_nvcr secrets/forge/tokens)
        export API_ENDPOINT="https://api.stg.ngc.nvidia.com"
        export FORGE_URL="https://forge.stg.ngc.nvidia.com/"
        export NGC_AUTH="https://stg.auth.ngc.nvidia.com/token?service=ngc&scope=group/ngc-stg"
        export PROVIDER_ORG="forge-prime-provider"
        export PROVIDER_ORG_ID="wdksahew1rqv"
        export TENANT_ORG="forge-tenant-dev"
        export TENANT_ORG_ID="fh93zk6uqtt1"
      else
        echo "ERROR: SITE_UNDER_TEST \"$SITE_UNDER_TEST\" not supported (expected pdx-demo1 or reno-dev4)" && exit 1
      fi
    - echo "[GENERAL]" > machine.config
    - echo "api_endpoint = \"$API_ENDPOINT\"" >> machine.config
    - echo "forge_url = \"$FORGE_URL\"" >> machine.config
    - echo "ngc_auth = \"$NGC_AUTH\"" >> machine.config
    - echo "[GENERAL.ProviderAdmin]" >> machine.config
    - echo "org = \"$PROVIDER_ORG\"" >> machine.config
    - echo "org_id = \"$PROVIDER_ORG_ID\"" >> machine.config
    - echo "token = \"$API_TOKEN\"" >> machine.config
    - echo "[GENERAL.TenantAdmin]" >> machine.config
    - echo "org = \"$TENANT_ORG\"" >> machine.config
    - echo "org_id = \"$TENANT_ORG_ID\"" >> machine.config
    - echo "token = \"$API_TOKEN\"" >> machine.config
    - echo "[ENV]" >> machine.config
    - echo "site = \"$SITE_UNDER_TEST\"" >> machine.config
    - export ENV="$(python -c 'import toml; print(toml.load("machine.config"))')"
  script:
    # Terminate the test suite after 1.5h in case of test hangs. This way the logs are still exported by after_script.
    - /usr/bin/dumb-init -- timeout --signal=SIGTERM 1.5h /root/entrypoint.sh
  after_script:
    # Clean up the execution log by stripping out the ANSI escape codes
    - sed -i -r "s/\x1B\[[0-9;]*[JKmsu]//g" /root/.tesla_linux_auto/task_run/TR1/execution.log
    - mv /root/.tesla_linux_auto/task_run ${CI_PROJECT_DIR}
  artifacts:
    paths:
      - task_run/
    when: always

#
# Run the Machine Lifecycle Test
#
scheduled-test:machine-lifecycle-test:
  stage: test
  timeout: 5h
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && $AUTO_DEPLOY == "false" && $RUN_TESTS =="true"
      needs:
        - job: scheduled-test:prepare
    - if: $CI_PIPELINE_SOURCE == "schedule" && $AUTO_DEPLOY == "true" && $RUN_TESTS =="false"
      when: never
    - if: $CI_PIPELINE_SOURCE == "schedule" && $AUTO_DEPLOY == "true" && $RUN_TESTS == "true"
      needs:
        - job: scheduled-test:prepare
        - job: scheduled-test:verify-site-controller-version
  retry:
    max: 0
  image:
    name: $CONTAINER_MACHINE_LIFECYCLE_TEST
    entrypoint: [ "" ]
  variables:
    CLIENT_CERT_PATH: certs/ci-api-user.crt
    CLIENT_KEY_PATH: certs/ci-api-user
    VAULT_NAMESPACE: ngc-forge
  id_tokens:
    VAULT_JWT_TOKEN:
      aud: $VAULT_ADDR
  before_script:
    - set -euo pipefail
    - export VAULT_JWT_ROLE=$VAULT_JWT_ROLE_FORGE
    - export VAULT_JWT_PATH=$VAULT_JWT_PATH_FORGE
#    - wget --no-verbose https://stg.ngc.nvidia.com/downloads/ngccli_linux.zip  # Fails with 403 in CI
    - export NGC_APPS_URL="https://api.stg.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions"
    - export NGC_CLI_LATEST="$(wget -qO- ${NGC_APPS_URL} | jq -r .recipe.latestVersionIdStr)"
    - wget --no-verbose ${NGC_APPS_URL}/${NGC_CLI_LATEST}/files/ngccli_linux.zip
    - unzip -q ngccli_linux.zip && chmod u+x ngc-cli/ngc
    - ln -s $(pwd)/ngc-cli/ngc /usr/local/bin/
    - export CARBIDE_VERSION=$(forge-admin-cli --format json -c https://${CARBIDE_API_URL} version | jq -r '.build_version')
    - echo "CARBIDE_VERSION=${CARBIDE_VERSION}" > env.sh
    - echo "Current carbide build deployed on ${SHORT_SITE_NAME} is ${CARBIDE_VERSION}"
  script:
    - source /tests/venv/bin/activate
    - python3 $CI_PROJECT_DIR/tests/machine-lifecycle/machine_lifecycle_test.py
  after_script:
    - export VAULT_JWT_ROLE=$VAULT_JWT_ROLE_FORGE
    - export VAULT_JWT_PATH=$VAULT_JWT_PATH_FORGE
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$VAULT_JWT_TOKEN)"
    - export GITLAB_API_TOKEN="$(vault kv get -field scheduled_pipeline_bot_token secrets/forge/tokens)"
    - source env.sh
    - |
      SNIPPET_URL="${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/snippets/7706"
      curl -s --request GET --header "PRIVATE-TOKEN: ${GITLAB_API_TOKEN}" --output orig_list.txt \
        "${SNIPPET_URL}/raw"

      if [[ ${CI_JOB_STATUS} == 'success' ]]; then
        echo "Test passed for build ${CARBIDE_VERSION} with ${DPU_COUNT} DPU(s). Adding result to snippet list..."
        new_list=$(echo "${CARBIDE_VERSION} (pipeline ${CI_PIPELINE_ID}) DPU_COUNT=${DPU_COUNT} - PASS" | cat - orig_list.txt | sed ':a;N;$!ba;s/\n/\\n/g')
        curl -s --request PUT --header "PRIVATE-TOKEN: ${GITLAB_API_TOKEN}" --header 'Content-Type: application/json' \
          --data "{\"files\": [{ \"action\":\"update\", \"file_path\":\"test_results\", \"content\":\"$new_list\" }]}" \
          "${SNIPPET_URL}"
      fi
  artifacts:
    when: always
    paths:
      - ngc_cli_debug.log

#
# Runs a test that spins up a single-node Forge control plane (i.e. a dev environment) directly on the GitLab runner and
# checks the health status of all kubernetes resources.
# Intended to be run as a pre-merge test on each MR. Can also be run against trunk.
# Guide here: https://gitlab-master.nvidia.com/nvmetal/ci-tools/internal-services-images/-/blob/main/README.md
#
# TODO: Expand test to cover connecting to an Acorn managed-host
#
dev-env-test:
  stage: test
  tags:
    - dev-env-carbide
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && $DEV_ENV_TEST == "true"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      changes:
        - '.skaffold/**/*'
        - 'api/**/*'
        - 'bluefield/**/*'
        - 'common/**/*'
        - 'dev/**/*'
        - 'dhcp/**/*'
        - 'dhcp-server/**/*'
        - 'dns/**/*'
        - 'health/**/*'
        - 'include/**/*'
        - 'pxe/**/*'
        - 'rpc/**/*'
        - 'scout/**/*'
        - '.envrc'
        - 'Cargo.lock'
        - 'Cargo.toml'
        - 'justfile'
        - 'Makefile.toml'
        - 'rust-toolchain.toml'
        - 'skaffold.yml'
      when: manual
      allow_failure: true  # To prevent blocking merge
  needs:
    - job: export-docker-json-config
  variables:
    VAULT_NAMESPACE: ngc-forge
    DEBUG_JUST: true
    LOG_ARGO: true
    DPU_NAME: no-dpu  # TODO: Set value when we get an Acorn managed-host to include in test
  id_tokens:
    VAULT_JWT_TOKEN:
      aud: $VAULT_ADDR
  before_script:
    - set -euo pipefail
    - mkdir -p ~/.docker
    - cp .docker/config.json ~/.docker/config.json
    - export VAULT_JWT_ROLE=$VAULT_JWT_ROLE_FORGE
    - export VAULT_JWT_PATH=$VAULT_JWT_PATH_FORGE
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$VAULT_JWT_TOKEN)"
  script:
    - .gitlab/dev-env-test.sh
  after_script:
    - set -euo pipefail
    - cp /tmp/argo_watch.log .
    - cp /tmp/argo_logs.log .
    - cd ../forged
    - source /root/.bashrc
    - eval "$(direnv export bash)"
    - just clean
  artifacts:
    when: always
    paths:
      - argo_logs.log
      - argo_watch.log
