#
# NOTE: Any new jobs not related to the scheduled testing pipeline should have the following
# included as the first rule to prevent them being pulled in to the scheduled pipeline:
#   - if: $CI_PIPELINE_SOURCE == "schedule"
#     when: never

variables:
  NSPECT_ID: NSPECT-K66V-JJ0K
  DOCKER_IMG: nvmetal-carbide


  FORGE_ADMIN_CLI_IMAGE: nvcr.io/nvidian/nvforge-devel/forge-admin-cli
  APPLICATION_DOCKER_IMAGE: nvcr.io/nvidian/nvforge-devel/nvmetal-carbide
  ARTIFACTS_DOCKER_IMAGE_AARCH64: nvcr.io/nvidian/nvforge-devel/boot-artifacts-aarch64
  ARTIFACTS_DOCKER_IMAGE_X86_64: nvcr.io/nvidian/nvforge-devel/boot-artifacts-x86_64

  APPLICATION_DOCKER_IMAGE_PRODUCTION: nvcr.io/nvidian/nvforge/nvmetal-carbide
  ARTIFACTS_DOCKER_IMAGE_AARCH64_PRODUCTION: nvcr.io/nvidian/nvforge/boot-artifacts-aarch64
  ARTIFACTS_DOCKER_IMAGE_X86_64_PRODUCTION: nvcr.io/nvidian/nvforge/boot-artifacts-x86_64

  FORGE_ADMIN_CLI_DOCKER_IMAGE: nvcr.io/nvidian/nvforge/forge-admin-cli

  FF_USE_FASTZIP: "true" # enable fastzip - a faster zip implementation that also supports level configuration.
  ARTIFACT_COMPRESSION_LEVEL: default # can also be set to fastest, fast, slow and slowest. If just enabling fastzip is not enough try setting this to fastest or fast.
  CACHE_COMPRESSION_LEVEL: fastest
  TRANSFER_METER_FREQUENCY: 5s

  KANIKO_CACHE_ARGS: "--cache=false --cache-copy-layers=true --cache-ttl=24h"
  APT_CACHE_DIR: $CI_PROJECT_DIR/apt
  LOGNAME: root
  KEA_BIN_PATH: /usr/bin
  KEA_INCLUDE_PATH: /usr/include/kea

  CARGO_INCREMENTAL: 0
  CARGO_HOME: $CI_PROJECT_DIR/cargo
  NEED_HELM: "yes"

  #
  # Container Regsitries for intermediate containers (build & runtime), not for publishing.
  #
  CONTAINER_REGISTRY_AARCH64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8"
  CONTAINER_REGISTRY_X86_64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86-64"

  
  # Canonical versions - will be overridden if you touch one of the dockerfiles that builds them
  #
  # How to update a build or runtime container.
  #
  # Make one commit where you update the Dockerfile in dev/docker.  Make an MR for it, it will build the new container and push it to urm in the MR.
  #
  # Next, *add* a commit to your MR where you update the relevant docker locations here.
  #
  # If you don't do the second step, merging to production will use the prior container.  I don't know how to solve this, but it's the way it was before
  #
  CONTAINER_BUILD_AARCH64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8/build-container:latest"
  CONTAINER_BUILD_X86_64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86-64/build-container:latest"
  CONTAINER_FORGE_ADMIN_CLI_URM: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/tools/forge-admin-cli:latest"
  CONTAINER_RUNTIME_AARCH64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8/runtime-container:latest"
  CONTAINER_RUNTIME_X86_64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86-64/runtime-container:latest"
  CONTAINER_BUILD_ARTIFACTS_AARCH64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8/build-artifacts-container:latest"
  CONTAINER_BUILD_ARTIFACTS_X86_64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86-64/build-artifacts-container:latest"
  CONTAINER_ARTIFACTS_X86_64: "alpine:latest"
  CONTAINER_ARTIFACTS_AARCH64: "alpine:latest"
  CONTAINER_SITE_AUTO_DEPLOY: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/tools/site-auto-deploy-container:latest"

include:
  - project: "nvmetal/ci-tools/build-container-cocogitto"
    ref: main
    file:
      - defaults.gitlab-ci.yml
  - project: "nvmetal/gitlab-templates"
    ref: main
    file:
      - defaults.gitlab-ci.yml
  - project: pstooling/gitlab-templates
    ref: main
    file: templates/pulse-in-pipeline/ContainerScan.gitlab-ci.yml
  - project: 'pstooling/gitlab-templates'
    ref: main
    file:
      - 'templates/pulse-in-pipeline/Scan.gitlab-ci.yml'

prepare-ci:
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH || $CI_COMMIT_BRANCH =~ /^release\//
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      needs:
        - job: "lint:commit-merge-request"
    - if: $CI_PIPELINE_SOURCE != "merge_request_event"
#    - if: $CI_PIPELINE_SOURCE == "push"
#      needs:
#        - job: "lint:commit-push"
#
# Reenable this when we want to enforce commit ci post-mr
#
lint:commit-push:
  rules:
    - when: never


build:
  rules:
    - when: never

build-push-helm:
  rules:
    - when: never

build-push-kustomize:
  rules:
    - when: never

notify-security-report:
  rules:
    - when: never

prepare-kustomize-for-scan:
  rules:
    - when: never

build-container:
  rules:
    - when: never

checkmarx-scan-csv:
  rules:
    - when: never

checkov-scan:
  rules:
    - when: never

container-scan:
  rules:
    - when: never

pulse-open-source:
  rules:
    - when: never

export-docker-json-config:
  when: always

#
# Build the container that builds our software
#
build-build-container-x86_64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  extends: build-container
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.build-container-x86_64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/x86-64/build-container
  after_script:
    - echo "CONTAINER_BUILD_X86_64=$CONTAINER_REGISTRY_X86_64/build-container:$VERSION" > build-container-x86_64.env
    - echo "CONTAINER_BUILD_X86_64_WAS_BUILT=true" >> build-container-x86_64.env
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.build-container-x86_64
  artifacts:
    reports:
      dotenv: build-container-x86_64.env

#
# ARM version uses kaniko instead of extending build-container because devops-tools doesn't have an ARM version
#
build-build-container-aarch64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.build-container-aarch64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/arm64v8/build-container
  before_script: |
    mkdir -p /kaniko/.docker
    echo "$DOCKER_AUTH_CONFIG" > /kaniko/.docker/config.json
  script: |
    set -xeuo pipefail
    /kaniko/executor --context $CI_PROJECT_DIR/dev/docker --dockerfile $DOCKER_FILE --destination $BUILD_DOCKER_IMAGE:$VERSION
  tags:
    - aarch64
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.build-container-aarch64
  after_script:
    - echo "CONTAINER_BUILD_AARCH64=$CONTAINER_REGISTRY_AARCH64/build-container:$VERSION" > build-container-aarch64.env
    - echo "CONTAINER_BUILD_AARCH64_WAS_BUILT=true" >> build-container-aarch64.env
  artifacts:
    reports:
      dotenv: build-container-aarch64.env

#
# Build the container that builds our software
#
build-artifacts-build-container-x86_64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  extends: build-container
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.build-artifacts-container-x86_64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/x86-64/build-artifacts-container
  after_script:
    - echo "CONTAINER_BUILD_ARTIFACTS_X86_64=$CONTAINER_REGISTRY_X86_64/build-artifacts-container:$VERSION" > build-artifacts-container-x86_64.env
    - echo "CONTAINER_BUILD_ARTIFACTS_X86_64_WAS_BUILT=true" >> build-artifacts-container-x86_64.env
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.build-artifacts-container-x86_64
  artifacts:
    reports:
      dotenv: build-artifacts-container-x86_64.env

#
# ARM version uses kaniko instead of extending build-container because devops-tools doesn't have an ARM version
#
build-artifacts-build-container-aarch64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.build-artifacts-container-aarch64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/arm64v8/build-artifacts-container
  before_script: |
    mkdir -p /kaniko/.docker
    echo "$DOCKER_AUTH_CONFIG" > /kaniko/.docker/config.json
  script: |
    set -xeuo pipefail
    /kaniko/executor --context $CI_PROJECT_DIR/dev/docker --dockerfile $DOCKER_FILE --destination $BUILD_DOCKER_IMAGE:$VERSION
  tags:
    - aarch64
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.build-artifacts-container-aarch64
  after_script:
    - echo "CONTAINER_BUILD_ARTIFACTS_AARCH64=$CONTAINER_REGISTRY_AARCH64/build-artifacts-container:$VERSION" > build-artifacts-container-aarch64.env
    - echo "CONTAINER_BUILD_ARTIFACTS_AARCH64_WAS_BUILT=true" >> build-artifacts-container-aarch64.env
  artifacts:
    reports:
      dotenv: build-artifacts-container-aarch64.env

#
# Build the container that will host the finalized carbide code.
#
build-runtime-container-x86_64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  extends: build-container
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.runtime-container-x86_64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/x86-64/runtime-container
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
    - changes:
        - dev/docker/Dockerfile.runtime-container-x86_64
  after_script:
    - echo "CONTAINER_RUNTIME_X86_64=$CONTAINER_REGISTRY_X86_64/runtime-container:$VERSION" > runtime-container-x86_64.env
    - echo "CONTAINER_RUNTIME_X86_64_WAS_BUILT==true" >> runtime-container-x86_64.env
  artifacts:
    reports:
      dotenv: runtime-container-x86_64.env

#
# ARM version uses kaniko instead of extending build-container because devops-tools doesn't have an ARM version
#
build-runtime-container-aarch64:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.runtime-container-aarch64
    BUILD_DOCKER_IMAGE: urm.nvidia.com/swngc-ngcc-docker-local/forge/$CI_PROJECT_NAME/arm64v8/runtime-container
  tags:
    - aarch64
  script: /bin/true
  rules:
    - when: never # TODO - we don't run carbide on ARM yet
#    - if: $CI_COMMIT_MESSAGE =~ /ci-rebuild-base-containers/
#    - changes:
#        - dev/docker/Dockerfile.runtime-container-aarch64
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
  after_script:
    - echo "CONTAINER_RUNTIME_AARCH64=$CONTAINER_REGISTRY_AARCH64/runtime-container:$VERSION" > runtime-container-aarch64.env
    - echo "CONTAINER_RUNTIME_AARCH64_WAS_BUILT=true" >> runtime-container-aarch64.env
  artifacts:
    reports:
      dotenv: runtime-container-aarch64.env

#
# When a new build/runtime/artifacts container is built, and we're on the
# trunk/main branch, then make sure we tag it with 'latest' so that subsequent
# jobs will use it.
#
tag-build-and-runtime-containers:
  stage: publish
  image:
    name: gcr.io/go-containerregistry/crane:debug
    entrypoint: [""]
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-build-container-x86_64
      optional: true
    - job: build-build-container-aarch64
      optional: true
    - job: build-runtime-container-x86_64
      optional: true
    - job: build-runtime-container-aarch64
      optional: true
    - job: build-artifacts-build-container-x86_64
      optional: true
    - job: build-artifacts-build-container-aarch64
      optional: true
  allow_failure: true
  before_script: |
    mkdir -p /root/.docker
    cp .docker/config.json /root/.docker/config.json
  script: |
    set -xeuo pipefail

    if [ ! -z "${CONTAINER_BUILD_X86_64_WAS_BUILT:-}" ]; then
      crane copy "${CONTAINER_BUILD_X86_64}" "${CONTAINER_REGISTRY_X86_64}/build-container:latest"
    fi

    if [ ! -z "${CONTAINER_BUILD_AARCH64_WAS_BUILT:-}" ]; then
      crane copy "${CONTAINER_BUILD_AARCH64}" "${CONTAINER_REGISTRY_AARCH64}/build-container:latest"
    fi

    if [ ! -z "${CONTAINER_BUILD_ARTIFACTS_X86_64_WAS_BUILT:-}" ]; then 
      crane copy "${CONTAINER_BUILD_ARTIFACTS_X86_64}" "${CONTAINER_REGISTRY_X86_64}/build-artifacts-container:latest"
    fi

    if [ ! -z "${CONTAINER_BUILD_ARTIFACTS_AARCH64_WAS_BUILT:-}" ]; then
      crane copy "${CONTAINER_BUILD_ARTIFACTS_AARCH64}" "${CONTAINER_REGISTRY_AARCH64}/build-artifacts-container:latest"
    fi

    if [ ! -z "${CONTAINER_RUNTIME_X86_64_WAS_BUILT:-}" ]; then
      crane copy "${CONTAINER_RUNTIME_X86_64}" "${CONTAINER_REGISTRY_X86_64}/runtime-container:latest"
    fi

    if [ ! -z "${CONTAINER_RUNTIME_AARCH64_WAS_BUILT:-}" ]; then
      crane copy  "${CONTAINER_RUNTIME_AARCH64}" "${CONTAINER_REGISTRY_AARCH64}/runtime-container:latest"
    fi

  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/ || $CI_COMMIT_BRANCH =~ /^release\//

#
# Build the final container that releases our code
#
build-release-container-x86_64:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
    - job: build-build-container-x86_64
      optional: true
    - job: build-runtime-container-x86_64
      optional: true
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-container-x86_64
  script:
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$CI_JOB_JWT)"
    - export GROUP_ACCESS_TOKEN=$(vault kv get -field GROUP_ACCESS_TOKEN secrets/forge/tokens)
    - docker build -t intermediate-container-$CI_COMMIT_SHORT_SHA -f $DOCKER_FILE --build-arg VERSION=$VERSION --build-arg CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA --build-arg CONTAINER_RUNTIME_X86_64=$CONTAINER_RUNTIME_X86_64 --build-arg CONTAINER_BUILD_X86_64=$CONTAINER_BUILD_X86_64 .
    - |
        set -euo pipefail
        if test "$DEPLOY_BUILT_CONTAINER" = "true"; then
          echo "Deploying build container: intermediate-container-$CI_COMMIT_SHORT_SHA to $APPLICATION_DOCKER_IMAGE:$VERSION"
          docker tag "intermediate-container-$CI_COMMIT_SHORT_SHA" "$APPLICATION_DOCKER_IMAGE:$VERSION"
          docker push "$APPLICATION_DOCKER_IMAGE:$VERSION"
          docker rmi "$APPLICATION_DOCKER_IMAGE:$VERSION"
        else
          echo "Skipping deployment of intermediate-container-$CI_COMMIT_SHORT_SHA"
        fi
    - docker rmi intermediate-container-$CI_COMMIT_SHORT_SHA
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH || $CI_COMMIT_BRANCH =~ /^release\//  # Run for default and those starting with "release/"
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"
  after_script:
    - echo "CONTAINER_RELEASE_X86_64=$APPLICATION_DOCKER_IMAGE:$VERSION" > release-container-x86_64.env
  artifacts:
    reports:
      dotenv: release-container-x86_64.env


build-container-forge-cli-x86_64:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-build-artifacts-container-x86_64
      optional: true
  before_script: |
    mkdir -p /kaniko/.docker
    cp .docker/config.json /kaniko/.docker/config.json
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-forge-cli
  dependencies:
    - prepare-ci
    - export-docker-json-config  
  tags:
    - x86_64
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  script:
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$CI_JOB_JWT)"
    - export GROUP_ACCESS_TOKEN=$(vault kv get -field GROUP_ACCESS_TOKEN secrets/forge/tokens)
    - >-
      /kaniko/executor --context "${CI_PROJECT_DIR}" --dockerfile "${DOCKER_FILE}" --build-arg "CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA" --build-arg "CONTAINER_BUILD=$CONTAINER_BUILD_ARTIFACTS_X86_64" --destination "${FORGE_ADMIN_CLI_IMAGE}-${VERSION}:amd64"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"

build-container-forge-cli-aarch64:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-build-artifacts-container-aarch64
      optional: true
  before_script: |
    mkdir -p /kaniko/.docker
    cp .docker/config.json /kaniko/.docker/config.json
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-forge-cli
  dependencies:
    - prepare-ci
    - export-docker-json-config  
  tags:
    - aarch64
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  script:
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$CI_JOB_JWT)"
    - export GROUP_ACCESS_TOKEN=$(vault kv get -field GROUP_ACCESS_TOKEN secrets/forge/tokens)
    - >-
      /kaniko/executor --context "${CI_PROJECT_DIR}" --dockerfile "${DOCKER_FILE}" --build-arg "CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA" --build-arg "CONTAINER_BUILD=$CONTAINER_BUILD_ARTIFACTS_AARCH64" --destination "${FORGE_ADMIN_CLI_IMAGE}-${VERSION}:arm64"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"

merge-manifests-forge-cli:
  stage: build
  # all containers must be build before merging them
  # alternatively the job may be configured to run in a later stage
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-container-forge-cli-aarch64
      artifacts: false
    - job: build-container-forge-cli-x86_64
      artifacts: false
  before_script: |
    mkdir -p /kaniko/.docker
    cp .docker/config.json /kaniko/.docker/config.json
  tags:
    # may run on any architecture supported by manifest-tool image
    - x86_64
  image:
    name: mplatform/manifest-tool:alpine
    entrypoint: [""]
  script:
    - >-
      manifest-tool --docker-cfg .docker/config.json push from-args --platforms linux/amd64,linux/arm64 --template "${FORGE_ADMIN_CLI_IMAGE}-${VERSION}:ARCH" --tags latest --target "${FORGE_ADMIN_CLI_IMAGE}:${VERSION}"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"

# Copy the final multi-arch container to urm for use in QA automation
copy-forge-admin-cli-to-urm:
  stage: build
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: merge-manifests-forge-cli
      optional: false
    - job: get-crane
      optional: false
  image:
    name: gcr.io/go-containerregistry/crane:debug
    entrypoint: [""]
  tags:
    - x86_64
  before_script:
    - mkdir -p /root/.docker
    - cp .docker/config.json /root/.docker/config.json
  script:
    - echo "Copying ${FORGE_ADMIN_CLI_IMAGE}:latest to ${CONTAINER_FORGE_ADMIN_CLI_URM}"
    - ./crane copy "${FORGE_ADMIN_CLI_IMAGE}:latest" "${CONTAINER_FORGE_ADMIN_CLI_URM}"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"


#
# Build the final container that releases our code
#
.build-boot-artifacts:
  stage: build
  variables:
    FORGE_CA: "prod"
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - when: always

build-boot-artifacts-x86_64:
  extends: .build-boot-artifacts
  needs:
    - job: prepare-ci
      optional: false
    - job: build-artifacts-build-container-x86_64
      optional: true
  image:
    name: $CONTAINER_BUILD_ARTIFACTS_X86_64
  script:
    - cargo make --cwd pxe build-boot-artifacts-x86_64-ci
  artifacts:
    paths:
      - pxe/static/blobs/internal/x86_64/ipxe.efi
      - target/debug/forge-admin-cli
      - target/debug/forge-scout
    when: always
    expire_in: 1 day

build-boot-artifacts-aarch64:
  extends: .build-boot-artifacts
  needs:
    - job: prepare-ci
      optional: false
    - job: build-artifacts-build-container-aarch64
      optional: true
  tags:
    - aarch64 # not shell because no mkosi aarch64 image
  image:
    name: $CONTAINER_BUILD_ARTIFACTS_AARCH64
  script:
    - cargo make --cwd pxe build-boot-artifacts-aarch64-ci
  artifacts:
    paths:
      - pxe/static/blobs/internal/aarch64/ipxe.efi
      - pxe/static/blobs/internal/aarch64/carbide.efi
      - pxe/static/blobs/internal/aarch64/carbide.root
      - pxe/static/blobs/internal/apt
      - pxe/static/blobs/internal/firmware
      - target/debug/forge-admin-cli
      - target/debug/forge-scout
    when: always
    expire_in: 1 day

# This just builds the mkosi container and uses artifacts from a previous build
build-boot-artifacts-ephemeral-image-x86-64:
  extends: .build-boot-artifacts
  needs:
    - job: prepare-ci
      optional: false
    - job: build-boot-artifacts-x86_64
      optional: false
  dependencies:
    - build-boot-artifacts-x86_64
  tags:
    - shell # mkosi cannot run in docker :(
    - ukify
    - mkosi
  image:
    name: $CONTAINER_BUILD_ARTIFACTS_AARCH64
  script:
    - cargo make --cwd pxe create-ephemeral-image-x86_64-ci
  after_script:
    - "sudo chown -R gitlab-runner: /home/gitlab-runner/builds/"
  artifacts:
    paths:
      - pxe/static/blobs/internal/x86_64/scout.efi
      - pxe/static/blobs/internal/x86_64/qcow-imager.efi
    when: always
    expire_in: 1 day

build-release-artifacts-x86_64:
  stage: build
  extends: build-push-container
  needs:
    - job: prepare-ci
      optional: false
    - job: build-boot-artifacts-x86_64
      optional: false
    - job: build-boot-artifacts-ephemeral-image-x86-64
      optional: false
  dependencies:
    - build-boot-artifacts-ephemeral-image-x86-64
    - build-boot-artifacts-x86_64
    - prepare-ci
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-artifacts-x86_64
  script:
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$CI_JOB_JWT)"
    - export GROUP_ACCESS_TOKEN=$(vault kv get -field GROUP_ACCESS_TOKEN secrets/forge/tokens)
    - docker build -t intermediate-artifacts-$CI_COMMIT_SHORT_SHA -f $DOCKER_FILE --build-arg VERSION=$VERSION --build-arg CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA --build-arg CONTAINER_RUNTIME_X86_64=$CONTAINER_ARTIFACTS_X86_64 .
    - |
        set -euo pipefail
        if test "$DEPLOY_BUILT_CONTAINER" = "true"; then
          echo "Deploying build container: intermediate-artifacts-$CI_COMMIT_SHORT_SHA to $ARTIFACTS_DOCKER_IMAGE_X86_64:$VERSION"
          docker tag "intermediate-artifacts-$CI_COMMIT_SHORT_SHA" "$ARTIFACTS_DOCKER_IMAGE_X86_64:$VERSION"
          docker push "$ARTIFACTS_DOCKER_IMAGE_X86_64:$VERSION"
        else
          echo "Skipping deployment of intermediate-artifacts-$CI_COMMIT_SHORT_SHA"
        fi
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/ || $CI_COMMIT_BRANCH =~ /^release\//
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"

build-release-artifacts-aarch64:
  stage: build
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  tags:
    - x86_64 # The AARCH64 artifacts are still packaged into the X86_64 version of the image because we execute commands in it on an X86_64 host
  needs:
    - job: prepare-ci
      optional: false
    - job: export-docker-json-config
      optional: false
    - job: build-runtime-container-x86_64
      optional: true
    - job: build-boot-artifacts-aarch64
      optional: false
  dependencies:
    - build-boot-artifacts-aarch64
    - prepare-ci
    - export-docker-json-config
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.release-artifacts-aarch64
  before_script: |
    mkdir -p /kaniko/.docker
    cp .docker/config.json /kaniko/.docker/config.json
  script: |
    set -euo pipefail
    if test "$DEPLOY_BUILT_CONTAINER" = "true"; then
      echo "Deploying container: $ARTIFACTS_DOCKER_IMAGE_AARCH64:$VERSION"

      # The AARCH64 artifacts are still packaged into the X86_64 version of the image because we execute commands in it on an X86_64 host
      /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $DOCKER_FILE --build-arg "CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA" --build-arg "CONTAINER_RUNTIME_AARCH64=$CONTAINER_ARTIFACTS_X86_64" --destination "$ARTIFACTS_DOCKER_IMAGE_AARCH64:$VERSION"
    else
      echo "Skipping deployment of $ARTIFACTS_DOCKER_IMAGE_AARCH64:$VERSION"

      # The AARCH64 artifacts are still packaged into the X86_64 version of the image because we execute commands in it on an X86_64 host
      /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $DOCKER_FILE --build-arg "CI_COMMIT_SHORT_SHA=$CI_COMMIT_SHORT_SHA" --build-arg "CONTAINER_RUNTIME_AARCH64=$CONTAINER_ARTIFACTS_X86_64" --no-push
    fi
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_MESSAGE =~ /ci-run-complete-pipeline/ || $CI_COMMIT_BRANCH =~ /^release\//
      variables:
        DEPLOY_BUILT_CONTAINER: "true"
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
      variables:
        DEPLOY_BUILT_CONTAINER: "false"

get-crane:
  stage: .pre
  image:
    name: gcr.io/go-containerregistry/crane:debug
    entrypoint: [""]
  variables:
    PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/busybox:/ko-app
  script:
    - cp $(which crane) .
  artifacts:
    paths:
      - crane
    when: always
    expire_in: 1 day
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - when: always

nspect-source-scan-x86_64:
  stage: .pre
  extends: .scan-with-report-no-fail
  variables:
    PULSE_NSPECT_ID: $NSPECT_ID
    PULSE_REPO_URL: "${CI_PROJECT_URL}.git"
    PULSE_SCAN_PROJECT: "carbide"
    PULSE_SCAN_PROJECT_VERSION: "${CI_COMMIT_BRANCH}"
    PULSE_SCAN_VULNERABILITY_REPORT: "nspect_${CI_COMMIT_BRANCH}_scan_report.json"
  rules:
    - when: never
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
      variables:
        PULSE_SCAN_OVERRIDE: "true"
        PULSE_REPO_BRANCH: trunk
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event' && $CI_COMMIT_MESSAGE =~ /ci-run-security-scan/

nspect-container-scan-x86_64:
  stage: security
  extends: .scan-dind-override-report
  services: []
  dependencies:
    - get-crane
    - export-docker-json-config
    - build-release-container-x86_64
  needs:
    - job: build-release-container-x86_64
      optional: false
    - job: get-crane
      optional: false
    - job: export-docker-json-config
      optional: false
  variables:
    CONTAINER_IMAGE: $CONTAINER_RELEASE_X86_64
  before_script:
    - mkdir -p /root/.docker
    - cp .docker/config.json /root/.docker/config.json
  script:
    - !reference [.scan_script, validate_requirements]
    - !reference [.scan_script, get_ssa_token]
    - echo "Using $CONTAINER_SCAN_HELPER_VERSION of the container scan helper."
    - ./crane version
    - ./crane pull ${CONTAINER_IMAGE} /tmp/archive.tar
    - pulse-cli -n $NSPECT_ID --ssa $SSA_TOKEN -p .gitlab/runtime-container-policy.json scan --override=true --program-version="Forge-LPR" --is-release=true -i /tmp/archive.tar --sbom json -o
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH|| $CI_COMMIT_BRANCH =~ /^release\//

promote-images-to-production:
  stage: promote
  image:
    name: gcr.io/go-containerregistry/crane:debug
    entrypoint: [""]
  needs:
    - job: build-release-artifacts-aarch64
      optional: false
    - job: build-release-artifacts-x86_64
      optional: false
    - job: build-release-container-x86_64
      optional: false
    - job: nspect-container-scan-x86_64
      optional: false
  when: manual
  before_script: |
    mkdir -p /root/.docker
    cp .docker/config.json /root/.docker/config.json
  script: |
    set -euo pipefail
    crane copy $APPLICATION_DOCKER_IMAGE:$VERSION $APPLICATION_DOCKER_IMAGE_PRODUCTION:$VERSION
    crane copy $ARTIFACTS_DOCKER_IMAGE_AARCH64:$VERSION $ARTIFACTS_DOCKER_IMAGE_AARCH64_PRODUCTION:$VERSION
    crane copy $ARTIFACTS_DOCKER_IMAGE_X86_64:$VERSION $ARTIFACTS_DOCKER_IMAGE_X86_64_PRODUCTION:$VERSION
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG || $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH || $CI_COMMIT_BRANCH =~ /^release\//

license-checks:
  stage: security
  image:
    name: $CONTAINER_BUILD_X86_64
  rules:
    - when: never
    - changes:
        - Cargo.lock
        - .license-config.yml
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - if: $CI_COMMIT_TAG
    - if: $CI_PIPELINE_SOURCE != 'merge_request_event'

pages:
  stage: doc
  needs:
    - job: export-docker-json-config
      optional: false
    - job: build-runtime-container-x86_64
      optional: true
    - job: build-boot-artifacts-aarch64
      optional: false
  image:
    name: $CONTAINER_BUILD_X86_64
  script:
    - cargo make book
  artifacts:
    paths:
      - public
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
    - changes:
        - book/
    - if: $CI_COMMIT_TAG
    - if: $CI_PIPELINE_SOURCE != 'merge_request_event'

#
# Build and push the image that contains the dependencies for job 'scheduled-test:auto-deploy-site-controller'
#
build-site-auto-deploy-container:
  extends: build-container
  rules:
    - changes:
        - dev/docker/Dockerfile.site-auto-deploy-container
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: never
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.site-auto-deploy-container
    BUILD_DOCKER_IMAGE: $CONTAINER_SITE_AUTO_DEPLOY
  script:
    - docker build --add-host=gitlab-master.nvidia.com:10.120.180.120 --file $DOCKER_FILE --tag $BUILD_DOCKER_IMAGE .
    - docker push $BUILD_DOCKER_IMAGE

#
# Get the latest versions of carbide and ssh-console from nvcr.io/nvidian/nvforge-devel
#
scheduled-test:get-latest-versions:
  needs:
    - job: export-docker-json-config
      optional: false
      artifacts: true
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: always
  image:
    name: golang:1.22
    entrypoint: [ "" ]
  variables:
    TESTABLE_VERSION_PATTERN: '^v\d{4}\.\d{2}-\w+(-\d+-g\w+)*$'
  before_script:
    - set -xeuo pipefail
    - mkdir -p /root/.docker
    - cp .docker/config.json /root/.docker/config.json
    - go install github.com/google/go-containerregistry/cmd/crane@v0.19.1
  script:
    - CARBIDE_VERSIONS="$(crane ls "${APPLICATION_DOCKER_IMAGE}" | grep -P "${TESTABLE_VERSION_PATTERN}" | sort -V | tail -n 10 | tac)"
    - BOOT_AARCH64_VERSIONS="$(crane ls "${ARTIFACTS_DOCKER_IMAGE_AARCH64}" | grep -P "${TESTABLE_VERSION_PATTERN}" | sort -V | tail -n 10 | tac)"
    - BOOT_X86_VERSIONS="$(crane ls "${ARTIFACTS_DOCKER_IMAGE_X86_64}" | grep -P "${TESTABLE_VERSION_PATTERN}" | sort -V | tail -n 10 | tac)"
    - |
      LATEST_COMMON_VERSION=""
      for version in $(echo "${CARBIDE_VERSIONS}"); do
        if [[ $(echo "${BOOT_AARCH64_VERSIONS}" | grep -c "\<$version\>") -gt 0 ]] && [[ $(echo "${BOOT_X86_VERSIONS}" | grep -c "\<$version\>") -gt 0 ]]; then
          LATEST_COMMON_VERSION=$version
          break
        fi
      done
      if [[ -z "${LATEST_COMMON_VERSION}" ]]; then
        echo "Error: There is no common version across the last 10 versions of nvmetal-carbide, boot-artifacts-aarch64 and boot-artifacts-x86_64."
        exit 1
      fi
    - SSH_CONSOLE_VERSION="$(crane ls "${DEV_NVCR_BASE}"/ssh-console | grep -P "${TESTABLE_VERSION_PATTERN}" | sort -V | tail -n 1)"
    - |
      if [[ -z "${SSH_CONSOLE_VERSION}" ]]; then
        echo "Error: SSH_CONSOLE_VERSION is empty."
        exit 1
      fi
    - echo "LATEST_COMMON_VERSION=${LATEST_COMMON_VERSION}" >> versions.env
    - echo "SSH_CONSOLE_VERSION=${SSH_CONSOLE_VERSION}" >> versions.env
  artifacts:
    reports:
      dotenv: versions.env

#
# Auto-deploy the latest versions of carbide and ssh-console to the site under test by updating the `forged` repo
#
scheduled-test:auto-deploy-site-controller:
  needs:
    - job: scheduled-test:get-latest-versions
      optional: false
      artifacts: true
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: always
  retry:
    when:
      - runner_system_failure
    max: 2
  image:
    name: $CONTAINER_SITE_AUTO_DEPLOY
    entrypoint: [ "" ]
  variables:
    # This variable is specified in GitLab UI > Scheduled Pipelines
    SITE_UNDER_TEST: $SITE_UNDER_TEST
    VAULT_ADDR: https://prod.vault.nvidia.com
    VAULT_NAMESPACE: ngc-forge
    GITLAB_SERVER_URL_NO_PORT: $CI_SERVER_PROTOCOL://$CI_SERVER_HOST
    FORGED_PROJECT_ID: 90150
    FORGED_BRANCH: "auto-update-from-carbide-pipeline-${CI_PIPELINE_IID}"
    FORGED_COMMIT_MSG: "chore: Auto-update ${SITE_UNDER_TEST} to site-controller ${LATEST_COMMON_VERSION}"
    ARGOCD_SITE_USERNAME: admin
  before_script:
    - set -euo pipefail
    # Cannot overwrite predefined GitLab variables using the 'variables' section, so do it here:
    - export VAULT_JWT_PATH="auth/jwt/nvidia/gitlab-master/login"
    - export VAULT_JWT_ROLE="gitlab-ci"
    - export VAULT_TOKEN="$(vault write -field=token "${VAULT_JWT_PATH}" role="${VAULT_JWT_ROLE}" jwt="${CI_JOB_JWT}")"
    - export ARGOCD_SITE_PASSWORD="$(vault kv get -field argocd-admin-password secrets/${SITE_UNDER_TEST})"
    # Parse short site name from full site name
    - export SHORT_SITE_NAME=$(echo "${SITE_UNDER_TEST}" | cut -d '-' -f 2)
    - echo "SHORT_SITE_NAME=${SHORT_SITE_NAME}" >> site.env
  script:
    - bash .gitlab/update-site-controller.sh
  artifacts:
    reports:
      dotenv: site.env
