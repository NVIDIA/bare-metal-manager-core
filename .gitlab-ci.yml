variables:

  NSPECT_ID: NSPECT-K66V-JJ0K
  APPLICATION_DOCKERFILE: dev/deployment/Dockerfile
  KUSTOMIZE_LOCATION: dev/kube
  DOCKER_IMG: nvmetal-carbide
  APPLICATION_DOCKER_IMAGE: quay.io/nvidia/nvmetal-carbide
  NVCR_APPLICATION_DOCKER_IMAGE: nvcr.io/nvidian/forge/nvmetal-carbide
  STG_NVCR_APPLICATION_DOCKER_IMAGE: stg.nvcr.io/nvidia/nvforge/nvmetal-carbide

  FF_USE_FASTZIP: "true" # enable fastzip - a faster zip implementation that also supports level configuration.
  ARTIFACT_COMPRESSION_LEVEL: default # can also be set to fastest, fast, slow and slowest. If just enabling fastzip is not enough try setting this to fastest or fast.
  CACHE_COMPRESSION_LEVEL: fastest
  TRANSFER_METER_FREQUENCY: 5s

  KANIKO_CACHE_ARGS: "--cache=false --cache-copy-layers=true --cache-ttl=24h"
  APT_CACHE_DIR: $CI_PROJECT_DIR/apt
  LOGNAME: root
  KEA_BIN_PATH: /usr/bin
  KEA_INCLUDE_PATH: /usr/include/kea

  CARGO_INCREMENTAL: 0
  CARGO_HOME: $CI_PROJECT_DIR/cargo
  NEED_HELM: "yes"

  # pull new versions from https://urm.nvidia.com/artifactory/swngc-ngcc-docker-local/forge/carbide/
  BUILD_CONTAINER_IMAGE_NAME_ARM: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8/build-container:v0.0.1-512-g3475afc9"
  BUILD_CONTAINER_IMAGE_NAME_X86_64: "urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86-64/build-container:v0.0.1-512-g3475afc9"
  BINARY_FILES: "binaries/carbide-api binaries/carbide-dns binaries/carbide binaries/carbide-admin-cli"

stages:
  - lint
  - build
  - test
  - publish
  - e2e-test
  - deploy
  - doc
  - security

include:
  - project: "nvmetal/gitlab-templates"
    ref: main
    file:
      - defaults.gitlab-ci.yml

nightly_tests:
  stage: e2e-test
  image:
    name: $TOOLS_IMAGE_REPO
  needs: [build-push-kustomize]
  rules:
    - if: $CI_DEFAULT_BRANCH == $CI_COMMIT_REF_NAME && $CI_PIPELINE_SOURCE == "schedule"
  script:
    - echo "Nothing to run at this moment"

build:
  rules:
    - when: never

produce-binary-artifacts:
  needs: [build-cargo]
  rules:
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
    - if: $CI_PIPELINE_SOURCE == "schedule"

build-container:
  variables:
    DOCKER_FILE_LOCATION: $CI_PROJECT_DIR/dev/docker
    DOCKER_FILE: Dockerfile.build-container
  rules:
    - changes:
        - dev/docker/Dockerfile.build-container

build-container-arm:
  stage: .pre
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  before_script: |
    mkdir -p /kaniko/.docker
    echo "$DOCKER_AUTH_CONFIG" > /kaniko/.docker/config.json
  script: |
    set -xeuo pipefail

    /kaniko/executor --context $CI_PROJECT_DIR/dev/docker --dockerfile $CI_PROJECT_DIR/dev/docker/Dockerfile.build-container-arm --destination urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/arm64v8/build-container:$VERSION
  needs:
    - prepare-ci
  tags:
    - aarch64
  rules:
    - changes:
        - dev/docker/Dockerfile.build-container-arm

build-container-helm:
  stage: .pre
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  before_script: |
    mkdir -p /kaniko/.docker
    echo "$DOCKER_AUTH_CONFIG" > /kaniko/.docker/config.json
  script: |
    set -xeuo pipefail

    /kaniko/executor --context $CI_PROJECT_DIR/dev/docker --dockerfile $CI_PROJECT_DIR/dev/docker/Dockerfile.helm --destination urm.nvidia.com/swngc-ngcc-docker-local/forge/carbide/x86_64/forge-helm:$VERSION
  needs:
    - prepare-ci
  rules:
    - changes:
        - dev/docker/Dockerfile.helm


# if you modify this, you must keep it in sync with ${PROJECT_ROOT}/dev/deployment/Dockerfile or you will notice post-merge build failures.
build-cargo:
  stage: build
  image:
    name: $BUILD_CONTAINER_IMAGE_NAME_X86_64
  variables:
    CARGO_MAKE_SKIP_CODECOV: "true"
    DATABASE_URL: "postgresql://%2Fvar%2Frun%2Fpostgresql"
    SCCACHE_DIR: /sccache
    RUSTC_WRAPPER: sccache
  script:
    - /etc/init.d/postgresql start
    - sudo -u postgres psql -c "ALTER USER root WITH SUPERUSER;"
    - createdb root
    - cargo make clippy-flow
    - cargo make check-format-flow
    - cargo make workspace-ci-flow
    - mkdir binaries
    - cp target/debug/carbide-api binaries/
    - cp target/debug/carbide-dns binaries/
    - cp target/debug/carbide binaries/
    - cp target/debug/carbide-admin-cli binaries
  rules:
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
    - if: $CI_PIPELINE_SOURCE == "schedule"
  artifacts:
    paths:
      - binaries
    expire_in: 1 day
  after_script:
    - cargo clean

# Test the container that build-push-container will use _after_ we merge.
#
# Copy-pasted from nvmetal/gitlab-templates/templates/pre.gitlab.ci.yml::build-container. Only DOCKER_FILE changes.
build-deployment-container:
  stage: build
  image:
    name: $TOOLS_IMAGE_REPO
  variables:
    VAULT_JWT_ROLE: forge-ci
    VAULT_ADDR: https://prod.vault.nvidia.com
    VAULT_NAMESPACE: ngc
    VAULT_JWT_PATH: auth/gitlab/login
    DOCKER_FILE: $APPLICATION_DOCKERFILE
    DOCKER_IMAGE: $BUILD_DOCKER_IMAGE
  before_script:
    - ./${CI_SCRIPTS_LOCATION}/scripts/get-docker-json.sh
  script:
    - export VAULT_TOKEN="$(vault write -field=token $VAULT_JWT_PATH role=$VAULT_JWT_ROLE jwt=$CI_JOB_JWT)"
    - export GROUP_ACCESS_TOKEN=$(vault kv get -field GROUP_ACCESS_TOKEN secrets/forge/tokens)
    - ./${CI_SCRIPTS_LOCATION}/scripts/build-push-container-image.sh
  needs:
    - prepare-ci
  tags:
    - x86_64
  when: manual

.build-boot-artifacts:
  stage: build
  before_script:
    - mkdir -p ~/.jfrog
    - echo "${JFROG_CONFIG}" > ~/.jfrog/jfrog-cli.conf.v5
  after_script:
    - "sudo chown -R gitlab-runner: /home/gitlab-runner/builds/"
 
build-boot-artifacts-x86_64:
  extends: .build-boot-artifacts
  variables:
    FORGE_CA: "prod"
  tags:
    - shell
  image:
    name: $BUILD_CONTAINER_IMAGE_NAME_X86_64
  script:
    - cargo make --cwd pxe build-boot-artifacts-x86_64-ci
  artifacts:
    paths:
      - pxe/static/blobs/internal/x86_64/
    when: always
    expire_in: 1 day
  rules:
    - if: $CI_COMMIT_TAG

build-boot-artifacts-aarch64:
  extends: .build-boot-artifacts
  variables:
    FORGE_CA: "prod"
  image:
    name: $BUILD_CONTAINER_IMAGE_NAME_ARM
  script:
    - cargo make --cwd pxe build-boot-artifacts-aarch64-ci
  tags:
    - aarch64
  rules:
    - if: $CI_COMMIT_TAG

build-boot-artifacts-nonprod-x86_64:
  extends: .build-boot-artifacts
  tags:
    - shell
  image:
    name: $BUILD_CONTAINER_IMAGE_NAME_X86_64
  script:
    - cargo make --cwd pxe build-boot-artifacts-x86_64-ci
  artifacts:
    paths: 
      - pxe/static/blobs/internal/x86_64/
    when: always
    expire_in: 1 day
  rules:
    - if: '$CI_COMMIT_BRANCH == "trunk"'

build-boot-artifacts-nonprod-aarch64:
  extends: .build-boot-artifacts
  image:
    name: $BUILD_CONTAINER_IMAGE_NAME_ARM
  script:
    - cargo make --cwd pxe build-boot-artifacts-aarch64-ci
  tags:
    - aarch64
  artifacts:
    paths:
      - pxe/static/blobs/internal/aarch64/
    when: always
    expire_in: 1 day
  rules:
    - if: '$CI_COMMIT_BRANCH == "trunk"'

.prepare-boot-artifacts-ci:
  stage: .pre
  image:
    name: alpine:latest
  variables:
    AUTH_FILE: ${CI_PROJECT_DIR}/kaniko/.docker/config.json
    BUILDTITLE: "forge-boot-artifacts"
  before_script:
    - apk add jq curl bash git
  script:
    - ./dev/prep-build-containers-ci.sh
  artifacts:
    paths:
      - kaniko/
    reports:
      dotenv: environment.env
    expire_in: 30 minutes
  rules:
    - if: '$CI_COMMIT_BRANCH == "trunk"'
    - if: $CI_COMMIT_TAG

prepare-ci-build-push-boot-artifacts-aarch64:
  extends: .prepare-boot-artifacts-ci
  variables:
    CPU_ARCH: aarch64

prepare-ci-build-push-boot-artifacts-x86_64:
  extends: .prepare-boot-artifacts-ci

.build-push-boot-artifacts-container:
  stage: publish
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  variables:
    DOCKER_FILE: $CI_PROJECT_DIR/dev/docker/Dockerfile.boot-artifacts
  before_script:
    - cp -r ${CI_PROJECT_DIR}/kaniko/.docker /kaniko
  script: |
    set -xeuo pipefail
    /kaniko/executor --context $CI_PROJECT_DIR --dockerfile ${DOCKER_FILE} $IMAGE_LABELS --destination $DOCKER_IMAGE:$VERSION
 
build-push-boot-artifacts-container-nonprod-aarch64:
  extends: .build-push-boot-artifacts-container
  variables:
    DOCKER_IMAGE: stg.nvcr.io/nvidia/nvforge/boot-artifacts-aarch64
  needs:
    - prepare-ci-build-push-boot-artifacts-aarch64
    - build-boot-artifacts-nonprod-aarch64
  tags:
    - aarch64
  dependencies:
    - prepare-ci-build-push-boot-artifacts-aarch64
    - build-boot-artifacts-nonprod-aarch64
  rules:
    - if: '$CI_COMMIT_BRANCH == "trunk"'

build-push-boot-artifacts-container-aarch64:
  extends: .build-push-boot-artifacts-container
  variables:
    DOCKER_IMAGE: nvcr.io/nvidian/forge/boot-artifacts-aarch64
  needs:
    - prepare-ci-build-push-boot-artifacts-aarch64
    - build-boot-artifacts-aarch64
  tags:
    - aarch64
  dependencies:
    - prepare-ci-build-push-boot-artifacts-aarch64
    - build-boot-artifacts-aarch64
  rules:
    - if: $CI_COMMIT_TAG

build-push-boot-artifacts-container-x86_64:
  extends: .build-push-boot-artifacts-container
  variables:
    DOCKER_IMAGE: nvcr.io/nvidian/forge/boot-artifacts-x86_64
  tags:
    - x86_64
  needs:
    - build-boot-artifacts-x86_64
    - prepare-ci-build-push-boot-artifacts-x86_64
  dependencies:
    - build-boot-artifacts-x86_64
    - prepare-ci-build-push-boot-artifacts-x86_64
  rules:
    - if: $CI_COMMIT_TAG

build-push-boot-artifacts-container-nonprod-x86_64:
  extends: .build-push-boot-artifacts-container
  variables:
    DOCKER_IMAGE: stg.nvcr.io/nvidia/nvforge/boot-artifacts-x86_64
  needs:
    - build-boot-artifacts-nonprod-x86_64
    - prepare-ci-build-push-boot-artifacts-x86_64
  tags:
    - x86_64
  dependencies:
    - build-boot-artifacts-nonprod-x86_64
    - prepare-ci-build-push-boot-artifacts-x86_64
  rules:
    - if: '$CI_COMMIT_BRANCH == "trunk"'

license-checks:
  before_script:
    - /bin/bash -l -c "CARGO_HOME=/root/.cargo rustup toolchain uninstall stable-x86_64-unknown-linux-gnu"
    - /bin/bash -l -c "CARGO_HOME=/root/.cargo rustup toolchain install stable-x86_64-unknown-linux-gnu"
  rules:
    - changes:
        - Cargo.lock
        - .license-config.yml
    - if: $CI_COMMIT_TAG
    - if: $CI_PIPELINE_SOURCE != 'merge_request_event'

checkmarx-scan-csv:
  before_script: |
    set -e
    /app/check_variables.sh gitlab
    rm -f pxe/mkosi.extra/etc/systemd/system/multi-user.target.wants/carbide-discovery.service

    # The VPC project will be scanned as part of the VPC repo, there is no need to run the scan
    # for VPC as part of the carbide repository
    rm -rf vpc

    # iPXE findings are inactionable - https://nvidia.slack.com/archives/C02RKLCN8BT/p1658425245618589?thread_ts=1658424844.339609&cid=C02RKLCN8BT
    rm -rf pxe/ipxe

    # Request to exclude mkosi from scans - https://nvidia.slack.com/archives/C02RKLCN8BT/p1658424426970489
    rm -rf pxe/mkosi

pages:
  stage: doc
  image:
    name: $BUILD_CONTAINER_IMAGE_NAME_X86_64
  script:
    - cargo make book
  artifacts:
    paths:
      - public
  only:
    - trunk

build-push-helm:
  variables:
    PUSH_TO_NVCR: "true"
    HELM_FILE_NAME: "${CI_PROJECT_NAME}-${HELM_VERSION}"
  before_script:
    - apt-get update
    - apt-get install -y gettext-base
    - export LOCALBIN=$PWD/"ci/bin"
    - mkdir -p $LOCALBIN
    - GOBIN=$LOCALBIN go install github.com/vinodchitraliNVIDIA/helmify/cmd/helmify@v0.0.1
    - GOBIN=$LOCALBIN go install sigs.k8s.io/kustomize/kustomize/v4@v4.5.7
    - $LOCALBIN/kustomize build dev/kube/overlays/helm | $LOCALBIN/helmify $CI_PROJECT_NAME
    - yq -i '.carbideApiDeployment.carbideApi.image.tag = env(VERSION)' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbideApiDeployment.carbideApi.image.repository = "nvcr.io/nvidian/forge/nvmetal-carbide"' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbideDnsDeployment.carbideDns.image.tag = env(VERSION)' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbideDnsDeployment.carbideDns.image.repository = "nvcr.io/nvidian/forge/nvmetal-carbide"' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbidePxeDeployment.carbidePxe.image.tag = env(VERSION)' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbidePxeDeployment.carbidePxe.image.repository = "nvcr.io/nvidian/forge/nvmetal-carbide"' $CI_PROJECT_NAME/values.yaml
    - yq -i '.keaDhcpDeployment.keaDhcp.image.tag = env(VERSION)' $CI_PROJECT_NAME/values.yaml
    - yq -i '.keaDhcpDeployment.keaDhcp.image.repository = "nvcr.io/nvidian/forge/nvmetal-carbide"' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbideApiDeployment.envoy.image.tag = "35f8d59b1172a85f02dd119791a9ef42c728cfa8"' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbideApiDeployment.envoy.image.repository = "nvcr.io/nvidian/forge/envoy-dev"' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbidePxeDeployment.envoy.image.tag = "35f8d59b1172a85f02dd119791a9ef42c728cfa8"' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbidePxeDeployment.envoy.image.repository = "nvcr.io/nvidian/forge/envoy-dev"' $CI_PROJECT_NAME/values.yaml
    - yq -i '.version = env(HELM_VERSION)' $CI_PROJECT_NAME/Chart.yaml
    - yq -i '.carbide-db-migrations.image.repository = "nvcr.io/nvidian/forge/nvmetal-carbide"' $CI_PROJECT_NAME/values.yaml
    - yq -i '.carbide-db-migrations.image.tag = env(VERSION)' $CI_PROJECT_NAME/values.yaml
    - export URM_USERNAME=$(devopsctl credentials get forge/urm username)
    - export URM_PASSWORD=$(devopsctl credentials get forge/urm password)
    - yq -i '.artifactsCreds.artifactToken = env(URM_PASSWORD)' $CI_PROJECT_NAME/values.yaml
    - yq -i '.artifactsCreds.artifactUser = env(URM_USERNAME)' $CI_PROJECT_NAME/values.yaml
    - sed -i 's|quay.io/nvidia/nvmetal-carbide:latest|{{ index .Values "carbide-db-migrations" "image" "repository" }}:{{ index .Values "carbide-db-migrations" "image" "tag" }}|g' $CI_PROJECT_NAME/templates/carbide-db-migrations.yaml
    - |
      sed -i 's/name: artifacts-creds/name: {{ include "carbide.fullname" . }}-artifacts-creds/g' $CI_PROJECT_NAME/templates/deployment.yaml
      sed -i '/^spec:/a \ \ imagePullSecrets:\n \ \- name: imagepullsecret' $CI_PROJECT_NAME/templates/postgres.yaml
      sed -i 's/carbide-postgres-pguser-carbide-postgres/{{ .Release.Name }}-postgres-pguser-{{ .Release.Name }}-postgres/g' $CI_PROJECT_NAME/templates/*
      sed -i 's/secretName: forge-provisioner-cert/secretName: {{ include "carbide.fullname" . }}-forge-provisioner-cert/g' $CI_PROJECT_NAME/templates/*
      sed -i 's/nvcr\.io\/nvidian\/forge\/crunchy-pgbackrest:ubi8-2\.38-0/{{ .Values.postGresBt.image }}/g' $CI_PROJECT_NAME/templates/postgres.yaml
      sed -i 's/nvcr\.io\/nvidian\/forge\/crunchy-postgres:ubi8-14\.2-1/{{ .Values.postGres.image }}/g' $CI_PROJECT_NAME/templates/postgres.yaml
      yq -i '.postGresBt.image = "nvcr.io/nvidian/forge/crunchy-pgbackrest:ubi8-2.38-0"' $CI_PROJECT_NAME/values.yaml
      yq -i '.postGres.image = "nvcr.io/nvidian/forge/crunchy-postgres:ubi8-14.2-1"' $CI_PROJECT_NAME/values.yaml
      sed -i 's/@type/"@type"/g' $CI_PROJECT_NAME/templates/*
      helm package $CI_PROJECT_NAME/

deploy_dev1:
  extends: .deploy_kustomize
  environment:
    name: dev1
    deployment_tier: development

deploy_dev2:
  extends: .deploy_kustomize
  environment:
    name: dev2
    deployment_tier: development

deploy_qa:
  extends: .deploy_kustomize
  environment:
    name: qa
    deployment_tier: testing
  tags:
    - corp

deploy_forge_singlenode_01:
  extends: .deploy_kustomize
  environment:
    name: forge-singlenode-01
    deployment_tier: testing
  tags:
    - corp

deploy_nbu:
  extends: .deploy_kustomize
  environment:
    name: nbu_testing
    deployment_tier: testing
  tags:
    - corp

deploy_dogfood:
  extends: .deploy_helm
  environment:
    name: dogfood
    deployment_tier: testing
  tags:
    - corp

deploy_fleet_command_site_0:
  extends: .deploy_fleet_command
  before_script:
    - export HELM_VER="${HELM_VERSION:1}"
  variables:
    APP_NAME: carbide
    HELM_CHART_NAME: carbide
    HELM_VER: ${HELM_VER}
    DEPLOYMENT_NAME: carbide
    LOCATION: site-0
    TARGET_NAMESPACE: forge-system
  environment:
    name: fleet_command
    deployment_tier: testing
  tags:
    - corp

deploy_fleet_command_integration_env:
  extends: .deploy_fleet_command
  before_script:
    - export HELM_VER="${HELM_VERSION:1}"
  variables:
    APP_NAME: carbide
    HELM_CHART_NAME: carbide
    HELM_VER: ${HELM_VER}
    DEPLOYMENT_NAME: carbide
    LOCATION: reno-int-lp
    TARGET_NAMESPACE: carbide
    ENV_NAME: stg
    ORG_NAME: nvidia
    TEAM_NAME: nvforge
  environment:
    name: fleet_command
    deployment_tier: testing
  tags:
    - corp
